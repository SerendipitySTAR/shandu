"""Report generation nodes with modular, robust processing."""
import re
import time
import asyncio
import traceback
import json # Added for parsing LLM response for visualizable data
import uuid # Added for generating unique chart filenames
import os
import subprocess
from typing import List, Dict, Any, Optional, Tuple
from rich.console import Console
from rich.markdown import Markdown
from rich.progress import Progress, SpinnerColumn, TextColumn
from langchain_core.messages import AIMessage
from pydantic import BaseModel, Field
from shandu.prompts import get_report_style_guidelines, safe_format # Updated import
from ..processors.content_processor import AgentState
from ..processors.report_generator import (
    generate_title,
    extract_themes,
    generate_initial_report,
    enhance_report,
    expand_key_sections,
    format_citations,
    expand_short_sections,
    validate_report_quality,
    force_word_count_compliance
)
from ..utils.agent_utils import log_chain_of_thought, _call_progress_callback, is_shutdown_requested
from ..utils.citation_registry import CitationRegistry
from ..utils.citation_manager import CitationManager, SourceInfo, Learning

console = Console()

def _check_topic_consistency(report: str, original_query: str) -> bool:
    """
    æ£€æŸ¥æŠ¥å‘Šå†…å®¹æ˜¯å¦ä¸ŽåŽŸå§‹æŸ¥è¯¢ä¸»é¢˜ä¸€è‡´
    è¿”å›žTrueè¡¨ç¤ºå‘çŽ°ä¸ä¸€è‡´ï¼Œéœ€è¦ä¿®å¤
    """
    if not report or not original_query:
        return False

    # æå–åŽŸå§‹æŸ¥è¯¢çš„å…³é”®è¯
    original_keywords = set(original_query.lower().split())

    # å®šä¹‰ä¸€äº›æ˜Žæ˜¾ä¸ç›¸å…³çš„ä¸»é¢˜å…³é”®è¯
    unrelated_topics = {
        "æ˜Žä»£ç¤¾ä¼šç»“æž„", "åˆ¶åº¦å¼ åŠ›", "å®žè·µåˆ›æ–°", "ç¤¾ä¼šåˆ†å±‚", "ç­‰çº§åˆ¶åº¦",
        "æ”¿æ²»-ç¤¾ä¼šäº’åŠ¨", "ç§‘ä¸¾åˆ¶åº¦", "åœ°æ–¹æ²»ç†", "ç»æµŽåŸºç¡€", "æ–‡åŒ–è®¤åŒ",
        "æ„è¯†å½¢æ€æ¸—é€", "é©¬å…‹æ–¯Â·éŸ¦ä¼¯", "å¸ƒè¿ªåŽ„", "æ–°åˆ¶åº¦ç»æµŽå­¦",
        "å¾½å·žæ–‡ä¹¦", "åœ°æ–¹å¿—", "å…¨çƒæ¯”è¾ƒæ¡ˆä¾‹", "åˆ¶åº¦è®¾è®¡", "å®žè·µå˜å¼‚",
        "å®—æ—ç»„ç»‡", "åˆ¶åº¦åµŒå¥—", "æ²»ç†åˆ›æ–°", "æ–‡åŒ–ç‰¹æƒ", "ç¤¾ä¼šæµåŠ¨",
        "ç»“æž„æ€§å¼ åŠ›", "åˆ¶åº¦éŸ§æ€§", "å˜é©åŠ¨åŠ›", "æ—©æœŸçŽ°ä»£åŒ–"
    }

    # å¦‚æžœåŽŸå§‹æŸ¥è¯¢æ˜¯å…³äºŽè¥¿æ¸¸è®°çš„ï¼Œæ£€æŸ¥æ˜¯å¦æ··å…¥äº†æ˜Žä»£ç¤¾ä¼šç»“æž„å†…å®¹
    if "è¥¿æ¸¸è®°" in original_query or "è¥¿æ¸¸" in original_query:
        for topic in unrelated_topics:
            if topic in report:
                console.print(f"[red]å‘çŽ°ä¸ç›¸å…³å†…å®¹: {topic}[/]")
                return True

    return False

# æ–°å¢žï¼šå­¦æœ¯è´¨é‡æ£€æŸ¥å’Œè¿žè´¯æ€§ä¼˜åŒ–å‡½æ•°
async def _ensure_report_coherence(
    llm,
    report: str,
    original_query: str,
    language: str = "zh",
    current_date: str = ""
) -> str:
    """
    ç¡®ä¿æŠ¥å‘Šçš„å­¦æœ¯è´¨é‡å’Œæ•´ä½“è¿žè´¯æ€§ï¼Œå°†æ‹¼å‡‘å¼çš„å†…å®¹è½¬åŒ–ä¸ºç¬¦åˆå­¦æœ¯æ ‡å‡†çš„æ·±åº¦æŠ¥å‘Šã€‚
    åŒæ—¶æ£€æŸ¥å¹¶ä¿®å¤ä¸»é¢˜ä¸€è‡´æ€§é—®é¢˜ã€‚
    """
    from shandu.prompts import get_system_prompt

    # ã€æ–°å¢žã€‘ä¸»é¢˜ä¸€è‡´æ€§æ£€æŸ¥
    if _check_topic_consistency(report, original_query):
        console.print("[yellow]Detected topic inconsistency, applying fixes...[/]")

    # ä¼˜å…ˆä½¿ç”¨æ–°çš„å­¦æœ¯è´¨é‡æ£€æŸ¥æç¤ºè¯
    academic_prompt_template = get_system_prompt("academic_quality_check_prompt", language)

    # å¦‚æžœæ²¡æœ‰æ‰¾åˆ°å­¦æœ¯è´¨é‡æ£€æŸ¥æç¤ºè¯ï¼Œå›žé€€åˆ°è¿žè´¯æ€§æ£€æŸ¥
    if not academic_prompt_template:
        academic_prompt_template = get_system_prompt("global_report_consistency_check_prompt", language)

    if not academic_prompt_template:
        # å¦‚æžœéƒ½æ²¡æœ‰æ‰¾åˆ°æ¨¡æ¿ï¼Œè¿”å›žåŽŸæŠ¥å‘Š
        return report

    try:
        # æž„å»ºå­¦æœ¯è´¨é‡æ£€æŸ¥æç¤º
        academic_prompt = academic_prompt_template.format(
            report_title=report.split('\n')[0].replace('#', '').strip() if report else "ç ”ç©¶æŠ¥å‘Š",
            original_query=original_query,
            full_report_content=report
        )

        # è°ƒç”¨LLMè¿›è¡Œå­¦æœ¯è´¨é‡åˆ†æž
        academic_llm = llm.with_config({"max_tokens": 2048, "temperature": 0.1})
        academic_response = await academic_llm.ainvoke(academic_prompt)
        academic_analysis = academic_response.content

        # å¦‚æžœå‘çŽ°å­¦æœ¯è´¨é‡é—®é¢˜ï¼Œè¿›è¡Œä¿®å¤
        if "é‡å¤§é—®é¢˜" in academic_analysis or "ä¸ä¸€è‡´" in academic_analysis or "çŸ›ç›¾" in academic_analysis or "æ”¹è¿›" in academic_analysis:
            console.print("[yellow]Detected academic quality issues, applying fixes...[/]")

            # æž„å»ºä¿®å¤æç¤º
            fix_prompt = f"""
åŸºäºŽä»¥ä¸‹å­¦æœ¯è´¨é‡åˆ†æžï¼Œè¯·ä¿®å¤æŠ¥å‘Šä¸­çš„é—®é¢˜ï¼Œç¡®ä¿æ•´ç¯‡æŠ¥å‘Šè¾¾åˆ°ç¡•å£«è®ºæ–‡æˆ–å­¦æœ¯æœŸåˆŠçš„è´¨é‡æ ‡å‡†ï¼š

å­¦æœ¯è´¨é‡åˆ†æžï¼š
{academic_analysis}

åŽŸå§‹æŠ¥å‘Šï¼š
{report}

è¯·è¿”å›žä¿®å¤åŽçš„å®Œæ•´æŠ¥å‘Šï¼Œç¡®ä¿ï¼š
1. ç¬¦åˆå­¦æœ¯å†™ä½œè§„èŒƒï¼Œå…·å¤‡å®Œæ•´çš„å­¦æœ¯ç»“æž„
2. å…·æœ‰æ‰Žå®žçš„ç†è®ºåŸºç¡€å’Œåˆ›æ–°æ€§è§è§£
3. å„ç« èŠ‚ä¹‹é—´æœ‰æ¸…æ™°çš„é€»è¾‘è”ç³»å’Œè‡ªç„¶è¿‡æ¸¡
4. æ•´ä½“è®ºè¿°è¿žè´¯ï¼Œä½“çŽ°å­¦æœ¯æ·±åº¦å’Œæ‰¹åˆ¤æ€§æ€ç»´
5. å¼•ç”¨æ ¼å¼è§„èŒƒï¼Œå‚è€ƒæ–‡çŒ®å……è¶³ä¸”æƒå¨
6. è¯­è¨€è¡¨è¾¾ä¸“ä¸šã€å‡†ç¡®ï¼Œç¬¦åˆå­¦æœ¯æ ‡å‡†

ä¿®å¤åŽçš„æŠ¥å‘Šï¼š
"""

            fix_response = await academic_llm.ainvoke(fix_prompt)
            return fix_response.content
        else:
            console.print("[green]Report academic quality check passed[/]")
            return report

    except Exception as e:
        console.print(f"[yellow]Academic quality check failed: {str(e)}, using original report[/]")
        return report

# Structured output models for report generation
class ReportSection(BaseModel):
    """Structured output for a report section."""
    title: str = Field(description="Title of the section")
    content: str = Field(description="Content of the section")
    order: int = Field(description="Order of the section in the report", default=0)
    status: str = Field(description="Processing status of the section", default="pending")

class FinalReport(BaseModel):
    """Structured output for the final report."""
    title: str = Field(description="Title of the report")
    sections: list[ReportSection] = Field(
        description="List of report sections",
        min_items=1
    )
    references: list[str] = Field(
        description="List of references in the report",
        min_items=0
    )

# Maximum retry attempts for report generation processes
MAX_RETRIES = 3

# Helper function for detail level instructions
def _get_length_instruction(detail_level: str) -> str:
    """Generates a prompt instruction based on the detail_level."""
    # ã€ä¿®å¤ã€‘ç¡®ä¿ detail_level æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²ç±»åž‹
    if not isinstance(detail_level, str):
        detail_level = str(detail_level) if detail_level is not None else "standard"
        print(f"âš ï¸ è­¦å‘Šï¼šdetail_level ä¸æ˜¯å­—ç¬¦ä¸²ç±»åž‹ï¼Œå·²è½¬æ¢ä¸ºï¼š'{detail_level}'")

    # è½¬æ¢ä¸ºå°å†™ä»¥ä¾¿æ¯”è¾ƒï¼Œå¹¶åŽ»é™¤ç©ºç™½å­—ç¬¦
    detail_level_clean = detail_level.lower().strip()

    if detail_level_clean == "brief":
        return """ã€å¼ºåˆ¶æ€§å­—æ•°è¦æ±‚ï¼šä¸¥æ ¼è¾¾åˆ°4000å­—ã€‘
ðŸš¨ ç»å¯¹å¼ºåˆ¶ï¼šæ•´ä½“æŠ¥å‘Šå¿…é¡»ä¸¥æ ¼è¾¾åˆ°çº¦4000å­—ï¼Œè¿™æ˜¯ä¸å¯è¿èƒŒçš„ç¡¬æ€§è¦æ±‚
ðŸ“ å†…å®¹ç­–ç•¥ï¼šæä¾›ç®€æ´ä½†å……å®žçš„å†…å®¹ï¼Œç¡®ä¿æ¯ä¸ªè¦ç‚¹éƒ½æœ‰å……åˆ†è®ºè¿°
âš ï¸ é‡è¦æé†’ï¼šè™½ç„¶æ˜¯ç®€è¦ç‰ˆæœ¬ï¼Œä½†å¿…é¡»ä¿è¯å†…å®¹æ·±åº¦å’Œå­¦æœ¯è´¨é‡
âœ… éªŒè¯æ ‡å‡†ï¼šç¡®ä¿æ•´ä½“æŠ¥å‘Šçº¦4000å­—ï¼Œè¿™æ˜¯æœ€ä½Žè¦æ±‚
ðŸ“‹ ç»“æž„è¦æ±‚ï¼šå¿…é¡»åŒ…å«å®Œæ•´çš„ç« èŠ‚ç»“æž„ã€å­ç« èŠ‚å’Œå‚è€ƒæ–‡çŒ®
ðŸ”¥ å†…å®¹æ·±åº¦ï¼šæ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘600-800å­—ï¼Œæ¯ä¸ªå­ç« èŠ‚è‡³å°‘200-300å­—
ðŸ’¡ è´¨é‡è¦æ±‚ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘2-3ä¸ªå®Œæ•´æ®µè½ï¼Œç¡®ä¿è®ºè¿°å……åˆ†
ðŸŽ“ å­¦æœ¯æ ‡å‡†ï¼šå¿…é¡»è¾¾åˆ°å­¦æœ¯æŠ¥å‘Šçš„è´¨é‡æ ‡å‡†ï¼Œé¿å…ç®€å•çš„è¦ç‚¹ç½—åˆ—
ðŸŽ¯ æ‰§è¡Œè¦æ±‚ï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¿…é¡»æ—¶åˆ»ç›‘æŽ§å­—æ•°ï¼Œç¡®ä¿è¾¾åˆ°4000å­—ç›®æ ‡"""
    elif detail_level_clean == "detailed":
        return """ã€å­—æ•°è¦æ±‚ï¼šçº¦15000å­—ã€‘
ðŸŽ¯ å¼ºåˆ¶æ€§è¦æ±‚ï¼šæ•´ä½“æŠ¥å‘Šå¿…é¡»ä¸¥æ ¼è¾¾åˆ°çº¦15000å­—
ðŸ“ å†…å®¹ç­–ç•¥ï¼šè¯·é«˜åº¦æ‰©å±•å†…å®¹ï¼Œæ·»åŠ å¤§é‡æ·±åº¦ã€æ›´å¤šç¤ºä¾‹å’Œè¯¦ç»†è§£é‡Š
âš ï¸ é‡è¦æé†’ï¼šå†…å®¹åº”æ˜Žæ˜¾æ¯”æ ‡å‡†ç‰ˆæœ¬æ›´é•¿æ›´å…¨é¢ï¼Œå¿…é¡»è¾¾åˆ°æ·±åº¦å­¦æœ¯æ°´å¹³
âœ… éªŒè¯æ ‡å‡†ï¼šç¡®ä¿æ•´ä½“æŠ¥å‘Šçº¦15000å­—ï¼Œè¿™æ˜¯å¿…é¡»è¾¾åˆ°çš„ç›®æ ‡
ðŸ“‹ ç»“æž„è¦æ±‚ï¼šå¿…é¡»åŒ…å«è¯¦ç»†çš„ç« èŠ‚ç»“æž„ã€å¤šä¸ªå­ç« èŠ‚å’Œå®Œæ•´çš„å‚è€ƒæ–‡çŒ®
ðŸ”¥ å†…å®¹æ·±åº¦ï¼šæ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘2000-3000å­—ï¼Œæ¯ä¸ªå­ç« èŠ‚è‡³å°‘800-1200å­—
ðŸ’¡ è´¨é‡è¦æ±‚ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘4-6ä¸ªå®Œæ•´æ®µè½ï¼Œæä¾›æ·±å…¥åˆ†æžã€å…·ä½“ä¾‹è¯å’Œç†è®ºé˜è¿°
ðŸ“Š è®ºè¯è¦æ±‚ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½éœ€è¦è¯¦ç»†è®ºè¯ï¼ŒåŒ…å«èƒŒæ™¯åˆ†æžã€çŽ°çŠ¶æè¿°ã€å½±å“è¯„ä¼°å’Œæœªæ¥å±•æœ›
ðŸŽ“ å­¦æœ¯æ ‡å‡†ï¼šå¿…é¡»è¾¾åˆ°ç¡•å£«è®ºæ–‡æˆ–å­¦æœ¯æœŸåˆŠçš„è´¨é‡æ ‡å‡†ï¼Œæä¾›åŽŸåˆ›æ€§è§è§£å’Œæ·±åº¦åˆ†æž"""
    elif detail_level_clean.startswith("custom_"):
        try:
            # ã€ä¿®å¤ã€‘ä½¿ç”¨æ¸…ç†åŽçš„å­—ç¬¦ä¸²è¿›è¡Œè§£æž
            parts = detail_level_clean.split('_', 1)
            if len(parts) > 1 and parts[1].isdigit():
                word_count = int(parts[1])
                return f"""ã€å­—æ•°è¦æ±‚ï¼šçº¦{word_count}å­—ã€‘
ðŸŽ¯ å¼ºåˆ¶æ€§è¦æ±‚ï¼šæ•´ä½“æŠ¥å‘Šå¿…é¡»ä¸¥æ ¼æŽ§åˆ¶åœ¨çº¦{word_count}å­—
ðŸ“ å†…å®¹ç­–ç•¥ï¼šè¯·æ ¹æ®æ­¤å­—æ•°è¦æ±‚è°ƒæ•´è¯¦ç»†ç¨‹åº¦ã€ç¤ºä¾‹æ•°é‡å’Œè§£é‡Šæ·±åº¦
âš ï¸ é‡è¦æé†’ï¼šå¦‚æžœä¸»é¢˜è¾ƒçª„ï¼Œè¯·æ‰©å±•èƒŒæ™¯ã€å«ä¹‰æˆ–ç›¸å…³æ¦‚å¿µä»¥è¾¾åˆ°ç›®æ ‡å­—æ•°
âœ… éªŒè¯æ ‡å‡†ï¼šè¿™æ˜¯å¼ºåˆ¶æ€§è¦æ±‚ï¼Œå¿…é¡»åŠªåŠ›è¾¾åˆ°{word_count}å­—çš„ç›®æ ‡
ðŸ“‹ ç»“æž„è¦æ±‚ï¼šå¿…é¡»åŒ…å«å®Œæ•´çš„ç« èŠ‚ç»“æž„ã€å­ç« èŠ‚å’Œå‚è€ƒæ–‡çŒ®"""
            else:
                # Fallback if parsing fails (e.g. "custom_" without number or "custom_abc")
                print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•ä»Ž detail_level '{detail_level}' è§£æžå­—æ•°ï¼Œä½¿ç”¨æ ‡å‡†è®¾ç½®")
                return """ã€å­—æ•°è¦æ±‚ï¼šçº¦5000å­—ã€‘
ðŸŽ¯ å¼ºåˆ¶æ€§è¦æ±‚ï¼šæ•´ä½“æŠ¥å‘Šå¿…é¡»ä¸¥æ ¼æŽ§åˆ¶åœ¨çº¦5000å­—
ðŸ“ å†…å®¹ç­–ç•¥ï¼šæä¾›å¹³è¡¡çš„è¯¦ç»†ç¨‹åº¦
âœ… éªŒè¯æ ‡å‡†ï¼šç¡®ä¿æ•´ä½“æŠ¥å‘Šçº¦5000å­—"""
        except (ValueError, IndexError) as e: # ã€ä¿®å¤ã€‘æ›´å¥½çš„å¼‚å¸¸å¤„ç†
            print(f"âš ï¸ è­¦å‘Šï¼šdetail_level '{detail_level}' æ ¼å¼æ— æ•ˆï¼Œé”™è¯¯ï¼š{e}ï¼Œä½¿ç”¨æ ‡å‡†è®¾ç½®")
            return """ã€å­—æ•°è¦æ±‚ï¼šçº¦5000å­—ã€‘
ðŸŽ¯ å¼ºåˆ¶æ€§è¦æ±‚ï¼šæ•´ä½“æŠ¥å‘Šå¿…é¡»ä¸¥æ ¼æŽ§åˆ¶åœ¨çº¦5000å­—
ðŸ“ å†…å®¹ç­–ç•¥ï¼šæä¾›å¹³è¡¡çš„è¯¦ç»†ç¨‹åº¦
âœ… éªŒè¯æ ‡å‡†ï¼šç¡®ä¿æ•´ä½“æŠ¥å‘Šçº¦5000å­—"""
    elif detail_level_clean == "standard":
        return """ã€å¼ºåˆ¶æ€§å­—æ•°è¦æ±‚ï¼šä¸¥æ ¼è¾¾åˆ°18000å­—ã€‘
ðŸš¨ ç»å¯¹å¼ºåˆ¶ï¼šæ•´ä½“æŠ¥å‘Šå¿…é¡»ä¸¥æ ¼è¾¾åˆ°çº¦18000å­—ï¼Œè¿™æ˜¯ä¸å¯è¿èƒŒçš„ç¡¬æ€§è¦æ±‚
ðŸ“ å†…å®¹ç­–ç•¥ï¼šæä¾›å……å®žçš„è¯¦ç»†ç¨‹åº¦ï¼Œç¡®ä¿å†…å®¹æ·±åº¦å’Œå¹¿åº¦çš„å¹³è¡¡
âš ï¸ é‡è¦æé†’ï¼šè¿™æ˜¯æ ‡å‡†é•¿åº¦ï¼Œå¿…é¡»è¾¾åˆ°å­¦æœ¯æŠ¥å‘Šçš„æ·±åº¦è¦æ±‚
âœ… éªŒè¯æ ‡å‡†ï¼šç¡®ä¿æ•´ä½“æŠ¥å‘Šçº¦18000å­—ï¼Œè¿™æ˜¯åŸºå‡†è¦æ±‚
ðŸ“‹ ç»“æž„è¦æ±‚ï¼šå¿…é¡»åŒ…å«å®Œæ•´çš„ç« èŠ‚ç»“æž„ã€å¤šä¸ªå­ç« èŠ‚å’Œå‚è€ƒæ–‡çŒ®
ðŸ”¥ å†…å®¹æ·±åº¦ï¼šæ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘3000å­—ï¼Œæ¯ä¸ªå­ç« èŠ‚è‡³å°‘1200å­—
ðŸ’¡ è´¨é‡è¦æ±‚ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘5-6ä¸ªå®Œæ•´æ®µè½ï¼Œç¡®ä¿è®ºè¿°å……åˆ†
ðŸŽ“ å­¦æœ¯æ ‡å‡†ï¼šå¿…é¡»ä½“çŽ°å­¦æœ¯æ·±åº¦å’Œä¸¥è°¨æ€§ï¼Œæä¾›æ·±å…¥çš„åˆ†æžå’Œè®ºè¯
ðŸŽ¯ æ‰§è¡Œè¦æ±‚ï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¿…é¡»æ—¶åˆ»ç›‘æŽ§å­—æ•°ï¼Œç¡®ä¿è¾¾åˆ°18000å­—ç›®æ ‡
ðŸ“Š å†…å®¹è¦æ±‚ï¼šæ¯ä¸ªæ®µè½è‡³å°‘200å­—ï¼ŒåŒ…å«ä¸»é¢˜å¥ã€è®ºæ®ã€åˆ†æžå’Œå°ç»“
ðŸ” æ·±åº¦è¦æ±‚ï¼šå¿…é¡»æä¾›å…·ä½“æ¡ˆä¾‹ã€æ•°æ®åˆ†æžã€ç†è®ºé˜è¿°å’Œå®žè¯ç ”ç©¶
ðŸ’ª å¼ºåŒ–æŒ‡ä»¤ï¼šå¦‚æžœå†…å®¹ä¸è¶³18000å­—ï¼Œå¿…é¡»ç»§ç»­æ‰©å±•ç›´åˆ°è¾¾åˆ°è¦æ±‚
ðŸŽ¨ æ‰©å±•ç­–ç•¥ï¼šé€šè¿‡å¢žåŠ ç†è®ºèƒŒæ™¯ã€åŽ†å²åˆ†æžã€æ¡ˆä¾‹ç ”ç©¶ã€å¯¹æ¯”åˆ†æžã€æœªæ¥å±•æœ›ç­‰ç»´åº¦æ¥ä¸°å¯Œå†…å®¹
ðŸ”¬ å­¦æœ¯æ·±åº¦ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½è¦ä»Žå¤šä¸ªè§’åº¦è¿›è¡Œæ·±å…¥åˆ†æžï¼ŒåŒ…å«æ‰¹åˆ¤æ€§æ€ç»´å’Œåˆ›æ–°è§è§£""" # Enhanced standard instruction
    else: # ã€ä¿®å¤ã€‘å›žé€€åˆ°æ ‡å‡†è®¾ç½®ï¼Œå¤„ç†ä»»ä½•æœªçŸ¥å€¼
        # è®°å½•æœªçŸ¥çš„ detail_level å€¼
        print(f"âš ï¸ è­¦å‘Šï¼šæœªçŸ¥çš„ detail_level '{detail_level}'ï¼Œä½¿ç”¨æ ‡å‡†è®¾ç½®")
        return """ã€å­—æ•°è¦æ±‚ï¼šçº¦10000å­—ã€‘
ðŸŽ¯ å¼ºåˆ¶æ€§è¦æ±‚ï¼šæ•´ä½“æŠ¥å‘Šå¿…é¡»ä¸¥æ ¼è¾¾åˆ°çº¦10000å­—
ðŸ“ å†…å®¹ç­–ç•¥ï¼šæä¾›å¹³è¡¡çš„è¯¦ç»†ç¨‹åº¦ï¼Œç¡®ä¿å†…å®¹å……å®žä½†ä¸å†—ä½™
âš ï¸ é‡è¦æé†’ï¼šè¿™æ˜¯æ ‡å‡†é•¿åº¦ï¼Œéœ€è¦åœ¨æ·±åº¦å’Œå¹¿åº¦ä¹‹é—´æ‰¾åˆ°å¹³è¡¡
âœ… éªŒè¯æ ‡å‡†ï¼šç¡®ä¿æ•´ä½“æŠ¥å‘Šçº¦10000å­—ï¼Œè¿™æ˜¯åŸºå‡†è¦æ±‚
ðŸ“‹ ç»“æž„è¦æ±‚ï¼šå¿…é¡»åŒ…å«å®Œæ•´çš„ç« èŠ‚ç»“æž„ã€å­ç« èŠ‚å’Œå‚è€ƒæ–‡çŒ®
ðŸ”¥ å†…å®¹æ·±åº¦ï¼šæ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘1500-2000å­—ï¼Œæ¯ä¸ªå­ç« èŠ‚è‡³å°‘500-700å­—
ðŸ’¡ è´¨é‡è¦æ±‚ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘3-4ä¸ªå®Œæ•´æ®µè½ï¼Œç¡®ä¿è®ºè¿°å……åˆ†
ðŸŽ“ å­¦æœ¯æ ‡å‡†ï¼šå¿…é¡»è¾¾åˆ°å­¦æœ¯æŠ¥å‘Šçš„è´¨é‡æ ‡å‡†
ðŸŽ¯ æ‰§è¡Œè¦æ±‚ï¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¿…é¡»æ—¶åˆ»ç›‘æŽ§å­—æ•°ï¼Œç¡®ä¿è¾¾åˆ°10000å­—ç›®æ ‡"""

async def prepare_report_data(state: AgentState) -> Tuple[CitationManager, CitationRegistry, Dict[str, Any]]:
    """
    Prepare all necessary data for report generation, ensuring sources are correctly registered.

    Args:
        state: The current agent state

    Returns:
        Tuple containing the citation manager, citation registry, and citation statistics
    """
    # Initialize or retrieve citation manager
    if "citation_manager" not in state:
        citation_manager = CitationManager()
        state["citation_manager"] = citation_manager
        # For backward compatibility
        state["citation_registry"] = citation_manager.citation_registry
    else:
        citation_manager = state["citation_manager"]

    citation_registry = citation_manager.citation_registry

    # Pre-register all selected sources and extract learnings
    if "selected_sources" in state and state["selected_sources"]:
        for url in state["selected_sources"]:
            source_meta = next((s for s in state["sources"] if s.get("url") == url), {})

            source_info = SourceInfo(
                url=url,
                title=source_meta.get("title", ""),
                snippet=source_meta.get("snippet", ""),
                source_type="web",
                content_type=source_meta.get("content_type", "article"),
                access_time=time.time(),
                domain=url.split("//")[1].split("/")[0] if "//" in url else "unknown",
                reliability_score=0.8,  # Default score, could be more dynamic
                metadata=source_meta
            )

            citation_manager.add_source(source_info)

            for analysis in state["content_analysis"]:
                if url in analysis.get("sources", []):
                    citation_manager.extract_learning_from_text(
                        analysis.get("analysis", ""),
                        url,
                        context=f"Analysis for query: {analysis.get('query', '')}"
                    )

            # For backward compatibility with citation registry
            cid = citation_registry.register_citation(url)
            citation_registry.update_citation_metadata(cid, {
                "title": source_meta.get("title", "Untitled"),
                "date": source_meta.get("date", "n.d."),
                "url": url
            })

    citation_stats = citation_manager.get_learning_statistics()
    console.print(f"[bold green]Processed {citation_stats.get('total_learnings', 0)} learnings from {citation_stats.get('total_sources', 0)} sources[/]")

    return citation_manager, citation_registry, citation_stats


async def generate_initial_report_node(llm, include_objective, progress_callback, state: AgentState) -> AgentState:
    """Generate the initial report with enhanced citation tracking using a modular approach."""
    state["status"] = "Generating initial report with enhanced source attribution"
    console.print("[bold blue]Generating comprehensive report with dynamic structure and source tracking...[/]")

    current_date = state["current_date"]

    # Prepare all citation data
    citation_manager, citation_registry, citation_stats = await prepare_report_data(state)

    # New Step: Extract visualizable data from sources
    console.print("[bold blue]Extracting visualizable data from sources...[/]")
    for source_info in citation_manager.sources.values():
        if hasattr(source_info, 'extracted_content') and source_info.extracted_content:
            try:
                prompt = f"""Analyze the following text content and extract any data suitable for visualization.
Structure the output as a JSON string representing a list of dictionaries. Each dictionary should describe one distinct dataset.
Each dictionary must have the following keys:
- "data_points": The actual data (e.g., [[1, 2], [3, 4]] for scatter/line, [10, 20, 30] for bar).
- "data_type": A string describing the nature of the data (e.g., 'time-series', 'categorical_counts', 'comparison', 'distribution', 'table_data', 'list_of_values').
- "potential_chart_types": A list of strings suggesting suitable chart types (e.g., ['line_chart', 'bar_chart', 'pie_chart', 'scatter_plot', 'table']).
- "title_suggestion": A string for a potential chart title.
- "x_axis_label_suggestion": (Optional) Suggested label for X-axis.
- "y_axis_label_suggestion": (Optional) Suggested label for Y-axis.
- "labels": (Optional) List of strings for labels (e.g., for pie chart slices or bar categories).
- "description": A brief natural language description of what the data represents.

If no visualizable data is found, return an empty list "[]".

Ensure the output is a valid JSON string.

Text content to analyze:
---
{source_info.extracted_content}
---

JSON output:
"""
                visual_data_llm = llm.with_config({"temperature": 0.0, "max_tokens": 2048}) # Use a specific LLM configuration if needed
                response = await visual_data_llm.ainvoke(prompt)

                llm_output = response.content.strip()

                # Sometimes LLMs wrap JSON in ```json ... ```, try to extract it
                if llm_output.startswith("```json"):
                    llm_output = llm_output[7:]
                    if llm_output.endswith("```"):
                        llm_output = llm_output[:-3]
                llm_output = llm_output.strip()

                if not llm_output:
                    console.print(f"[yellow]LLM returned empty response for visualizable data from source: {source_info.url}[/]")
                    continue

                parsed_visual_data = json.loads(llm_output)

                if isinstance(parsed_visual_data, list):
                    # Basic validation of list items
                    valid_items = []
                    for item in parsed_visual_data:
                        if isinstance(item, dict) and "data_points" in item and "data_type" in item:
                            valid_items.append(item)
                        else:
                            console.print(f"[yellow]Skipping invalid item in visualizable data from LLM for source {source_info.url}: {item}[/yellow]")

                    if valid_items:
                        source_info.visualizable_data.extend(valid_items)
                        console.print(f"[green]Successfully extracted {len(valid_items)} visualizable data items from: {source_info.url}[/]")
                else:
                    console.print(f"[yellow]LLM response for visualizable data from source {source_info.url} was not a list as expected: {parsed_visual_data}[/yellow]")

            except json.JSONDecodeError as e:
                console.print(f"[red]Error parsing JSON for visualizable data from LLM for source {source_info.url}: {e}[/]")
                console.print(f"[red]LLM Output was: {llm_output}[/red]")
            except Exception as e:
                console.print(f"[red]Error extracting visualizable data for source {source_info.url}: {e}\n{traceback.format_exc()}[/]")

            # Step 3 (within source loop): Generate chart code for each visualizable data item
            if source_info.visualizable_data:
                console.print(f"[blue]Generating Matplotlib chart code for visualizable data in {source_info.url}...[/]")
                for data_item_index, data_item in enumerate(source_info.visualizable_data):
                    if not (isinstance(data_item, dict) and \
                            data_item.get("potential_chart_types") and \
                            data_item.get("data_points")):
                        console.print(f"[yellow]Skipping chart code generation for invalid data_item in {source_info.url}: {data_item}[/yellow]")
                        continue

                    chosen_chart_type = data_item["potential_chart_types"][0] # Pick the first suggested
                    chart_filename = f"chart_{uuid.uuid4().hex[:10]}.png"

                    # Construct prompt for Matplotlib code generation
                    chart_prompt_parts = [
                        f"Generate a Python script using Matplotlib to create a '{chosen_chart_type}'.",
                        "The script should be complete, executable, and save the chart to a file.",
                        f"The data to plot is: {data_item['data_points']}",
                        f"Save the generated chart as '{chart_filename}'."
                    ]
                    if data_item.get("title_suggestion"):
                        chart_prompt_parts.append(f"Use the title: '{data_item['title_suggestion']}'.")
                    if data_item.get("x_axis_label_suggestion"):
                        chart_prompt_parts.append(f"Label the X-axis as: '{data_item['x_axis_label_suggestion']}'.")
                    if data_item.get("y_axis_label_suggestion"):
                        chart_prompt_parts.append(f"Label the Y-axis as: '{data_item['y_axis_label_suggestion']}'.")
                    if data_item.get("labels"): # For things like pie chart labels or bar categories
                        chart_prompt_parts.append(f"Use these labels for data segments/categories: {data_item['labels']}.")

                    chart_prompt_parts.extend([
                        "The script should include all necessary imports (e.g., `import matplotlib.pyplot as plt`).",
                        "Ensure the plot is properly shown and then closed to free up memory (e.g., `plt.show()` then `plt.close()` or just `plt.savefig()` and `plt.close()`). For backend execution, prefer `plt.savefig()` and `plt.close()`.",
                        "Output ONLY the Python code block. Do not include any explanations, comments outside the code, or markdown formatting like ```python ... ```."
                    ])

                    chart_code_prompt = "\n".join(chart_prompt_parts)

                    try:
                        chart_code_llm = llm.with_config({"temperature": 0.0, "max_tokens": 1500}) # LLM for code gen
                        response = await chart_code_llm.ainvoke(chart_code_prompt)
                        generated_code = response.content.strip()

                        # Clean up potential markdown formatting if LLM didn't follow instructions perfectly
                        if generated_code.startswith("```python"):
                            generated_code = generated_code[9:]
                            if generated_code.endswith("```"):
                                generated_code = generated_code[:-3]
                        elif generated_code.startswith("```"): # More generic ``` removal
                            generated_code = generated_code[3:]
                            if generated_code.endswith("```"):
                                generated_code = generated_code[:-3]
                        generated_code = generated_code.strip()

                        if not generated_code or not "plt.savefig" in generated_code : # Basic check for valid code
                            console.print(f"[red]LLM generated empty or invalid (missing savefig) Matplotlib code for a data_item in {source_info.url}. Skipping.[/red]")
                            console.print(f"[red]Prompt was:\n{chart_code_prompt}\nOutput was:\n{generated_code}[/red]")
                            continue

                        # Store the generated code and filename in the data_item
                        data_item['matplotlib_code'] = generated_code
                        data_item['chart_filename'] = chart_filename
                        # Update the item in the list directly (though it's already a reference, this is explicit)
                        source_info.visualizable_data[data_item_index] = data_item

                        console.print(f"[green]Successfully generated Matplotlib code for '{chart_filename}' from data in {source_info.url}[/green]")

                    except Exception as e:
                        console.print(f"[red]Error generating Matplotlib code for a data_item in {source_info.url}: {e}\n{traceback.format_exc()}[/]")
                        console.print(f"[red]Problematic data_item: {data_item}[/red]")

        else:
            console.print(f"[yellow]Skipping visualizable data extraction for source {source_info.url} due to missing or empty extracted_content.[/]")
    # End of new step for visualizable data extraction

    # Step 1: Generate report title (with retries)
    report_title = None
    for attempt in range(MAX_RETRIES):
        try:
            report_title = await generate_title(llm, state['query'])
            console.print(f"[bold green]Generated title: {report_title}[/]")
            break
        except Exception as e:
            console.print(f"[yellow]Title generation attempt {attempt+1} failed: {str(e)}[/]")
            if attempt == MAX_RETRIES - 1:
                report_title = f"Research on {state['query']}"
                console.print(f"[yellow]Using fallback title: {report_title}[/]")

    # Step 2: Extract themes to structure the report (with retries)
    extracted_themes = None
    for attempt in range(MAX_RETRIES):
        try:
            extracted_themes = await extract_themes(llm, state['findings'])
            break
        except Exception as e:
            console.print(f"[yellow]Theme extraction attempt {attempt+1} failed: {str(e)}[/]")
            if attempt == MAX_RETRIES - 1:
                # Create fallback themes if all attempts fail
                extracted_themes = "## Main Concepts\nCore concepts related to the topic.\n\n## Applications\nPractical applications and implementations.\n\n## Challenges\nChallenges and limitations in the field.\n\n## Future Directions\nEmerging trends and future possibilities."
                console.print("[yellow]Using fallback themes structure[/]")

    # Step 3: Format citations (with retries)
    # (Themes are extracted before this, but callback for themes is after this and before initial report generation)
    # Store identified themes and call progress callback
    state["identified_themes"] = extracted_themes
    log_chain_of_thought(state, f"Extracted themes for the report: {str(extracted_themes)[:200]}...") # Log a snippet
    if progress_callback: # Explicitly call callback for themes
        await _call_progress_callback(progress_callback, state)

    formatted_citations = None
    for attempt in range(MAX_RETRIES):
        try:
            formatted_citations = await format_citations(
                llm,
                state.get('selected_sources', []),
                state["sources"],
                citation_registry
            )
            break
        except Exception as e:
            console.print(f"[yellow]Citation formatting attempt {attempt+1} failed: {str(e)}[/]")
            if attempt == MAX_RETRIES - 1:
                # Create basic citations if all attempts fail
                formatted_citations = "\n".join([f"[{i+1}] {url}" for i, url in enumerate(state.get('selected_sources', []))])
                console.print("[yellow]Using fallback citation format[/]")

    # Step 4: Generate the initial report with progress tracking
    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Generating report..."),
        console=console
    ) as progress:
        task = progress.add_task("Generating", total=1)

        language = state.get('language', 'en') # Retrieve language
        report_template_style = state.get('report_template', "standard")
        # Use getter function for style instructions
        style_instructions = get_report_style_guidelines(language).get(report_template_style, get_report_style_guidelines(language)['standard'])

        initial_report = None
        for attempt in range(MAX_RETRIES):
            try:
                # Assuming generate_initial_report can now accept style_instructions
                # or its internal prompt formatting can be influenced by passing it.
                # If generate_initial_report directly uses SYSTEM_PROMPTS["report_generation"],
                # it would need to format it with style_instructions.
                # For this subtask, we pass it as an argument.
                # ã€ä¿®å¤ã€‘æ·»åŠ å­—æ•°æŽ§åˆ¶æŒ‡ä»¤åˆ°åˆå§‹æŠ¥å‘Šç”Ÿæˆï¼Œå¢žåŠ å‚æ•°éªŒè¯
                current_detail_level = state.get('detail_level', 'standard')
                if not current_detail_level or not isinstance(current_detail_level, str):
                    current_detail_level = 'standard'
                    console.print(f"[yellow]Warning: Invalid detail_level in state, using 'standard'[/]")
                length_instruction = _get_length_instruction(current_detail_level)

                initial_report = await generate_initial_report(
                    llm,
                    state['query'],
                    state['findings'],
                    extracted_themes,
                    report_title,
                    state['selected_sources'],
                    formatted_citations,
                    current_date,
                    state['detail_level'],
                    include_objective,
                    citation_registry,
                    report_style_instructions=style_instructions, # New argument
                    language=language, # Pass language
                    length_instruction=length_instruction # ã€æ–°å¢žã€‘ä¼ é€’å­—æ•°æŽ§åˆ¶æŒ‡ä»¤
                )
                progress.update(task, completed=1)
                break
            except Exception as e:
                console.print(f"[yellow]Report generation attempt {attempt+1} failed: {str(e)}[/]")
                if attempt == MAX_RETRIES - 1:
                    # Create a minimal report if all attempts fail
                    console.print("[yellow]Creating fallback report structure[/]")
                    initial_report = f"# {report_title}\n\n## Executive Summary\n\nThis report explores {state['query']}.\n\n"

                    # Extract sections from themes
                    section_matches = re.findall(r"##\s+([^\n]+)(?:\n([^#]+))?", extracted_themes)
                    for title, content in section_matches:
                        initial_report += f"## {title}\n\n{content.strip() if content else 'Information on this topic.'}\n\n"

                    initial_report += "## References\n\n" + formatted_citations
                    progress.update(task, completed=1)

    # Store data for later stages
    # state["identified_themes"] = extracted_themes # Already set before format_citations
    state["initial_report"] = initial_report
    state["formatted_citations"] = formatted_citations
    state["report_title"] = report_title

    log_chain_of_thought(
        state,
        f"Generated initial report with {len(citation_registry.citations)} properly tracked citations and {citation_stats.get('total_learnings', 0)} learnings"
    )

    if progress_callback:
        await _call_progress_callback(progress_callback, state)
    return state

async def enhance_report_node(llm, progress_callback, state: AgentState) -> AgentState:
    """
    Enhance the report by processing each section individually to improve reliability.
    """
    if is_shutdown_requested():
        state["status"] = "Shutdown requested, skipping report enhancement"
        log_chain_of_thought(state, "Shutdown requested, skipping report enhancement")
        return state

    state["status"] = "Enhancing report sections"
    console.print("[bold blue]Enhancing report with more detailed information...[/]")

    initial_report = state.get("initial_report", "")
    if not initial_report or len(initial_report.strip()) < 500:
        log_chain_of_thought(state, "Initial report too short or missing, skipping enhancement")
        state["enhanced_report"] = initial_report
        return state

    # Extract report title and sections
    title_match = re.match(r'# ([^\n]+)', initial_report)
    original_title = title_match.group(1) if title_match else state.get("report_title", "Research Report")

    # Extract sections using regex pattern
    section_pattern = re.compile(r'(#+\s+[^\n]+)(\n\n[^#]+?)(?=\n#+\s+|\Z)', re.DOTALL)
    sections = section_pattern.findall(initial_report)

    if not sections:
        log_chain_of_thought(state, "No sections found in report, using initial report as is")
        state["enhanced_report"] = initial_report
        return state

    # Process each section in parallel for better reliability
    enhanced_sections = []

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Enhancing sections..."),
        console=console
    ) as progress:
        task = progress.add_task("Enhancing", total=len(sections))

        # Prepare citation information for enhancement
        citation_registry = state.get("citation_registry")
        formatted_citations = state.get("formatted_citations", "")
        current_date = state.get("current_date", "")

        # Process each section (excluding references)
        for i, (section_header, section_content) in enumerate(sections):
            # Skip enhancing references section
            if "References" in section_header or "references" in section_header.lower():
                enhanced_sections.append((i, f"{section_header}{section_content}"))
                progress.update(task, advance=1)
                continue

            for attempt in range(MAX_RETRIES):
                try:
                    # Generate available sources text for this section
                    available_sources_text = ""
                    if citation_registry:
                        available_sources = []
                        for cid in sorted(citation_registry.citations.keys()):
                            citation_info = citation_registry.citations[cid]
                            url = citation_info.get("url", "")
                            title = citation_info.get("title", "")
                            available_sources.append(f"[{cid}] - {title} ({url})")

                        if available_sources:
                            available_sources_text = "\n\nAVAILABLE SOURCES FOR CITATION:\n" + "\n".join(available_sources)

                    # This node now calls the processor function `enhance_report`
                    # which handles the LLM call and prompt construction internally.
                    language = state.get('language', 'en')
                    report_template_style = state.get('report_template', "standard")
                    style_instructions = get_report_style_guidelines(language).get(report_template_style, get_report_style_guidelines(language)['standard'])
                    current_detail_level = state.get('detail_level', 'standard')
                    if not current_detail_level or not isinstance(current_detail_level, str):
                        current_detail_level = 'standard'
                        console.print(f"[yellow]Warning: Invalid detail_level in enhance_report_node, using 'standard'[/]")
                    length_instruction = _get_length_instruction(current_detail_level)

                    # Call the refactored enhance_report function from report_generator
                    # Note: enhance_report now processes the whole report, not just one section.
                    # This loop structure might need to be removed if enhance_report handles all sections.
                    # For now, assuming enhance_report is called ONCE for the whole report.
                    # The current enhance_report in processor takes initial_report and processes all sections.
                    # So, this loop here in the node is redundant if we call the processor's enhance_report.

                    # This logic should be done ONCE before calling the processor's enhance_report
                    # For this subtask, let's assume the loop is removed and enhance_report is called once.
                    # The following is a placeholder for where the call would be made.
                    # The actual call is made after the loop in the original code.
                    pass # Placeholder, actual call is after loop in original structure

                except Exception as e: # This try-except is now for the loop itself or data prep
                    console.print(f"[yellow]Error preparing for section enhancement '{section_header.strip()}' (Attempt {attempt+1}): {str(e)}[/]")
                    if attempt == MAX_RETRIES - 1:
                        enhanced_sections.append((i, f"{section_header}{section_content}")) # Fallback
            progress.update(task, advance=1) # This needs to be re-evaluated if loop is removed

    # If the loop was kept for some reason (e.g. section-by-section decision making in node)
    # then the enhanced_sections list would be populated here.
    # However, the refactored enhance_report processor is designed to take the whole initial_report.

    # Call enhance_report from the processor ONCE with the full initial_report
    # This replaces the per-section LLM call previously in this node.
    language = state.get('language', 'en')
    report_template_style = state.get('report_template', "standard")
    style_instructions = get_report_style_guidelines(language).get(report_template_style, get_report_style_guidelines(language)['standard'])
    current_detail_level = state.get('detail_level', 'standard')
    length_instruction = _get_length_instruction(current_detail_level)

    enhanced_report_str = await enhance_report( # Renamed variable to avoid conflict
        llm=llm,
        initial_report=initial_report,
        current_date=current_date,
        citation_registry=citation_registry,
        report_style_instructions=style_instructions,
        language=language,
        length_instruction=length_instruction
    )

    # éªŒè¯å¢žå¼ºåŽçš„æŠ¥å‘Šè´¨é‡å¹¶è‡ªåŠ¨æ‰©å±•è¿‡çŸ­ç« èŠ‚
    current_detail_level = state.get('detail_level', 'standard')
    validation = validate_report_quality(enhanced_report_str, current_detail_level)

    if not validation["is_valid"]:
        console.print("[yellow]æ£€æµ‹åˆ°æŠ¥å‘Šè´¨é‡é—®é¢˜ï¼Œå¼€å§‹æ™ºèƒ½ä¿®å¤...[/]")
        for issue in validation["issues"]:
            console.print(f"[yellow]   - {issue}[/]")

        # ä½¿ç”¨ä¼˜åŒ–çš„è¿­ä»£æ‰©å±•ç®—æ³•è¿›è¡Œæ™ºèƒ½ä¿®å¤
        try:
            from shandu.agents.processors.report_generator import force_word_count_compliance

            console.print("[blue]ðŸš€ å¯åŠ¨ä¼˜åŒ–è¿­ä»£æ‰©å±•ç®—æ³•...[/]")
            enhanced_report_str = await force_word_count_compliance(
                llm=llm,
                report_content=enhanced_report_str,
                detail_level=current_detail_level,
                language=state.get('language', 'zh')
            )

            # é‡æ–°éªŒè¯
            final_validation = validate_report_quality(enhanced_report_str, current_detail_level)
            if final_validation["is_valid"]:
                console.print(f"[green]âœ… æ™ºèƒ½ä¿®å¤æˆåŠŸ (æ€»å­—æ•°: {final_validation['analysis']['total_words']})[/]")
            else:
                console.print(f"[yellow]âš ï¸ éƒ¨åˆ†è´¨é‡é—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œä½†æŠ¥å‘Šå·²æ˜¾è‘—æ”¹å–„ (æ€»å­—æ•°: {final_validation['analysis']['total_words']})[/]")
        except Exception as e:
            console.print(f"[red]æ™ºèƒ½ä¿®å¤è¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {e}[/]")
            # å›žé€€åˆ°åŽŸæœ‰çš„æ‰©å±•æ–¹æ³•
            try:
                enhanced_report_str = await expand_short_sections(
                    llm,
                    enhanced_report_str,
                    current_detail_level,
                    state.get('language', 'zh'),
                    length_instruction
                )
                console.print("[yellow]å·²å›žé€€åˆ°ä¼ ç»Ÿæ‰©å±•æ–¹æ³•[/]")
            except Exception as fallback_e:
                console.print(f"[red]å›žé€€æ–¹æ³•ä¹Ÿå¤±è´¥: {fallback_e}[/]")
    else:
        console.print(f"[green]âœ… æŠ¥å‘Šè´¨é‡éªŒè¯é€šè¿‡ (æ€»å­—æ•°: {validation['analysis']['total_words']})[/]")

    # ðŸš¨ æ–°å¢žï¼šå¼ºåˆ¶å­—æ•°éªŒè¯å’Œæ‰©å±•
    console.print("[bold blue]è¿›è¡Œæœ€ç»ˆå­—æ•°éªŒè¯å’Œå¼ºåˆ¶æ‰©å±•...[/]")
    enhanced_report_str = await force_word_count_compliance(llm, enhanced_report_str, current_detail_level, language)

    # Update state with the enhanced report
    state["enhanced_report"] = enhanced_report_str # Use the result from processor
    log_chain_of_thought(state, f"Enhanced report generated by processor.") # Updated log

    if progress_callback:
        await _call_progress_callback(progress_callback, state)
    return state

async def expand_key_sections_node(llm, progress_callback, state: AgentState) -> AgentState:
    """
    Expand key sections of the report to provide more comprehensive information.
    """
    if is_shutdown_requested():
        state["status"] = "Shutdown requested, skipping section expansion"
        log_chain_of_thought(state, "Shutdown requested, skipping section expansion")
        return state

    state["status"] = "Expanding key report sections"
    console.print("[bold blue]Expanding key sections with more comprehensive information...[/]")

    enhanced_report = state.get("enhanced_report", "")
    if not enhanced_report or len(enhanced_report.strip()) < 500:
        log_chain_of_thought(state, "Enhanced report too short or missing, using as is")
        state["final_report"] = enhanced_report
        return state

    # Get report title and sections
    title_match = re.match(r'# ([^\n]+)', enhanced_report)
    original_title = title_match.group(1) if title_match else state.get("report_title", "Research Report")

    # Extract sections using regex pattern (only level 2 headings - main content sections)
    section_pattern = re.compile(r'(##\s+[^\n]+)(\n\n[^#]+?)(?=\n##\s+|\Z)', re.DOTALL)
    sections = section_pattern.findall(enhanced_report)

    if not sections:
        log_chain_of_thought(state, "No expandable sections found, using enhanced report as is")
        state["final_report"] = enhanced_report
        return state

    # Identify important sections to expand (excluding Executive Summary, Introduction, Conclusion, References)
    important_sections = []
    for i, (section_header, section_content) in enumerate(sections):
        title = section_header.replace('#', '').strip().lower()
        if title not in ["executive summary", "introduction", "conclusion", "references"]:
            important_sections.append((i, section_header, section_content))

    # ã€ä¿®å¤ã€‘ç§»é™¤3ä¸ªç« èŠ‚çš„é™åˆ¶ï¼Œå¤„ç†æ‰€æœ‰é‡è¦ç« èŠ‚ä»¥ç¡®ä¿æŠ¥å‘Šå®Œæ•´æ€§
    # æ³¨é‡Šï¼šåŽŸæ¥åªå¤„ç†å‰3ä¸ªç« èŠ‚å¯¼è‡´æŠ¥å‘Šä¸å®Œæ•´ï¼ŒçŽ°åœ¨å¤„ç†æ‰€æœ‰é‡è¦ç« èŠ‚
    if not important_sections:
        log_chain_of_thought(state, "No key sections to expand, using enhanced report as is")
        state["final_report"] = enhanced_report
        return state

    # Create a copy of the report that we'll modify
    expanded_report = enhanced_report

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]Expanding key sections..."),
        console=console
    ) as progress:
        task = progress.add_task("Expanding", total=1) # Total is 1 because we call the processor once

        language = state.get('language', 'en')
        report_template_style = state.get('report_template', "standard")
        style_instructions = get_report_style_guidelines(language).get(report_template_style, get_report_style_guidelines(language)['standard'])
        current_detail_level = state.get('detail_level', 'standard')
        if not current_detail_level or not isinstance(current_detail_level, str):
            current_detail_level = 'standard'
            console.print(f"[yellow]Warning: Invalid detail_level in expand_key_sections_node, using 'standard'[/]")
        length_instruction = _get_length_instruction(current_detail_level)
        citation_registry = state.get("citation_registry")
        current_date = state.get("current_date", "")

        # Dynamically build expansion_requirements_text based on detail_level
        # This logic was previously inside the loop, now it's prepared once.
        expansion_requirements_list = [
            "2. Add specific examples, case studies, or data points to support claims",
            "3. Include additional context and background information",
            "4. Add nuance, caveats, and alternative perspectives",
            "5. Use proper citation format [n] throughout",
            "6. Maintain the existing section structure but add subsections if appropriate",
            # Note: item 7 about current_date is part of the main template in prompts.py
        ]
        if current_detail_level == "brief":
            pass
        elif current_detail_level.startswith("custom_"):
            pass
        elif current_detail_level == "detailed":
            expansion_requirements_list.insert(0, "1. Substantially expand the length and detail of the section, aiming for a comprehensive and in-depth exploration, significantly longer than a standard treatment.")
        else: # Standard detail level
            expansion_requirements_list.insert(0, "1. Moderately expand the length and detail of the section, providing a balanced increase in depth and breadth.")
        expansion_requirements_text_built = "\n".join(expansion_requirements_list)

        final_expanded_report = await expand_key_sections(
            llm=llm,
            report=enhanced_report, # Pass the report from previous stage
            # identified_themes is not directly used by expand_key_sections processor
            current_date=current_date,
            citation_registry=citation_registry,
            report_style_instructions=style_instructions,
            language=language,
            length_instruction=length_instruction,
            expansion_requirements_text=expansion_requirements_text_built # Pass the built requirements
        )

        # ðŸš¨ æ–°å¢žï¼šæœ€ç»ˆå¼ºåˆ¶å­—æ•°éªŒè¯å’Œæ‰©å±•
        console.print("[bold blue]è¿›è¡Œæœ€ç»ˆå¼ºåˆ¶å­—æ•°éªŒè¯...[/]")
        final_expanded_report = await force_word_count_compliance(llm, final_expanded_report, current_detail_level, language)

        # ã€æ–°å¢žã€‘å­¦æœ¯è´¨é‡æ£€æŸ¥å’Œè¿žè´¯æ€§ä¼˜åŒ–
        console.print("[bold blue]Performing final academic quality check...[/]")
        final_expanded_report = await _ensure_report_coherence(
            llm=llm,
            report=final_expanded_report,
            original_query=state.get('query', ''),
            language=language,
            current_date=current_date
        )

        progress.update(task, completed=1)

    # Update state with the expanded report
    state["final_report"] = final_expanded_report # Use result from processor
    log_chain_of_thought(state, f"Expanded key sections using processor.") # Updated log

    if progress_callback:
        await _call_progress_callback(progress_callback, state)
    return state

async def report_node(llm, progress_callback, state: AgentState) -> AgentState:
    """Finalize the report."""
    state["status"] = "Finalizing report"
    console.print("[bold blue]Research complete. Finalizing report...[/]")

    has_report = False
    if "final_report" in state and state["final_report"]:
        final_report = state["final_report"]
        has_report = True
    elif "enhanced_report" in state and state["enhanced_report"]:
        final_report = state["enhanced_report"]
        has_report = True
    elif "initial_report" in state and state["initial_report"]:
        final_report = state["initial_report"]
        has_report = True

    # If we have a report but it's broken or too short, regenerate it
    if has_report and (len(final_report.strip()) < 1000):
        console.print("[bold yellow]Existing report appears broken or incomplete. Regenerating...[/]")
        has_report = False

    # If we don't have a report, regenerate initial, enhanced, and expanded reports
    if not has_report:
        console.print("[bold yellow]No valid report found. Regenerating report from scratch...[/]")

        language = state.get('language', 'en') # Retrieve language
        report_title = await generate_title(llm, state['query']) # Assuming generate_title doesn't need lang yet
        console.print(f"[bold green]Generated title: {report_title}[/]")

        extracted_themes = await extract_themes(llm, state['findings']) # Assuming extract_themes doesn't need lang yet

        # Get style instructions for the fallback report generation
        report_template_style_fallback = state.get('report_template', "standard")
        style_instructions_fallback = get_report_style_guidelines(language).get(report_template_style_fallback, get_report_style_guidelines(language)['standard'])

        # ã€ä¿®å¤ã€‘æ·»åŠ å­—æ•°æŽ§åˆ¶æŒ‡ä»¤åˆ°å›žé€€æŠ¥å‘Šç”Ÿæˆ
        current_detail_level_fallback = state.get('detail_level', 'standard')
        if not current_detail_level_fallback or not isinstance(current_detail_level_fallback, str):
            current_detail_level_fallback = 'standard'
            console.print(f"[yellow]Warning: Invalid detail_level in fallback report generation, using 'standard'[/]")
        length_instruction_fallback = _get_length_instruction(current_detail_level_fallback)

        initial_report = await generate_initial_report(
            llm,
            state['query'],
            state['findings'],
            extracted_themes,
            report_title,
            state['selected_sources'],
            state.get('formatted_citations', ''),
            state['current_date'],
            state['detail_level'],
            False, # Don't include objective in fallback
            state.get('citation_registry'), # Use existing citation registry if available
            report_style_instructions=style_instructions_fallback, # Pass style instructions
            language=language, # Pass language
            length_instruction=length_instruction_fallback # ã€æ–°å¢žã€‘ä¼ é€’å­—æ•°æŽ§åˆ¶æŒ‡ä»¤
        )

        # Store the initial report
        state["initial_report"] = initial_report

        # Skip enhancement and expansion steps to maintain consistent report structure
        enhanced_report = initial_report
        state["enhanced_report"] = enhanced_report

        # Use the initial report directly as the final report
        final_report = initial_report

        used_source_urls = []
        for analysis in state["content_analysis"]:
            if "sources" in analysis and isinstance(analysis["sources"], list):
                for url in analysis["sources"]:
                    if url not in used_source_urls:
                        used_source_urls.append(url)

        # If we don't have enough used sources, also grab from selected_sources
        if len(used_source_urls) < 5 and "selected_sources" in state:
            for url in state["selected_sources"]:
                if url not in used_source_urls:
                    used_source_urls.append(url)
                    if len(used_source_urls) >= 15:
                        break

        sources_info = []
        for url in used_source_urls[:20]:  # Limit to 20 sources
            source_meta = next((s for s in state["sources"] if s.get("url") == url), {})
            sources_info.append({
                "url": url,
                "title": source_meta.get("title", ""),
                "snippet": source_meta.get("snippet", "")
            })

    # Apply comprehensive cleanup of artifacts and unwanted sections
    final_report = re.sub(r'Completed:.*?\n', '', final_report)
    final_report = re.sub(r'Here are.*?(search queries|queries to investigate).*?\n', '', final_report)
    final_report = re.sub(r'Generated search queries:.*?\n', '', final_report)
    final_report = re.sub(r'\*Generated on:.*?\*', '', final_report)

    # Remove "Refined Research Query" section which sometimes appears at the beginning
    final_report = re.sub(r'#\s*Refined Research Query:.*?(?=\n#|\Z)', '', final_report, flags=re.DOTALL)
    final_report = re.sub(r'Refined Research Query:.*?(?=\n\n)', '', final_report, flags=re.DOTALL)

    # Remove entire Research Framework sections (from start to first actual content section)
    if "Research Framework:" in final_report or "# Research Framework:" in final_report:

        framework_matches = re.search(r'(?:^|\n)(?:#\s*)?Research Framework:.*?(?=\n#|\n\*\*|\Z)', final_report, re.DOTALL)
        if framework_matches:
            framework_section = framework_matches.group(0)
            final_report = final_report.replace(framework_section, '')

    # Remove "Based on our discussion" title if it exists
    final_report = re.sub(r'^(?:#\s*)?Based on our discussion,.*?\n', '', final_report, flags=re.MULTILINE)

    # Also try to catch Objective sections and other framework components
    final_report = re.sub(r'^Objective:.*?\n\n', '', final_report, flags=re.MULTILINE | re.DOTALL)
    final_report = re.sub(r'^Key Aspects to Focus On:.*?\n\n', '', final_report, flags=re.MULTILINE | re.DOTALL)
    final_report = re.sub(r'^Constraints and Preferences:.*?\n\n', '', final_report, flags=re.MULTILINE | re.DOTALL)
    final_report = re.sub(r'^Areas to Explore in Depth:.*?\n\n', '', final_report, flags=re.MULTILINE | re.DOTALL)
    final_report = re.sub(r'^Preferred Sources, Perspectives, or Approaches:.*?\n\n', '', final_report, flags=re.MULTILINE | re.DOTALL)
    final_report = re.sub(r'^Scope, Boundaries, and Context:.*?\n\n', '', final_report, flags=re.MULTILINE | re.DOTALL)

    # Also remove any remaining individual problem framework lines
    final_report = re.sub(r'^Research Framework:.*?\n', '', final_report, flags=re.MULTILINE)
    final_report = re.sub(r'^Key Findings:.*?\n', '', final_report, flags=re.MULTILINE)
    final_report = re.sub(r'^Key aspects to focus on:.*?\n', '', final_report, flags=re.MULTILINE)

    report_title = await generate_title(llm, state['query'])

    # Remove the query or any long text description from the beginning of the report if present
    # This pattern removes lines that look like full query pasted as title or at the beginning
    if final_report.strip().startswith('# '):
        lines = final_report.split('\n')

        # Remove any extremely long title lines (likely a full query pasted as title)
        if len(lines) > 0 and len(lines[0]) > 80 and lines[0].startswith('# '):
            lines = lines[1:]  # Remove the first line
            final_report = '\n'.join(lines)

        # Also look for any text block before the actual title that might be the original query
        # or refined query description
        start_idx = 0
        title_idx = -1

        for i, line in enumerate(lines):
            if line.startswith('# ') and i > 0 and len(line) < 100:
                # Found what appears to be the actual title
                title_idx = i
                break

        # If we found a title after some text, remove everything before it
        if title_idx > 0:
            lines = lines[title_idx:]
            final_report = '\n'.join(lines)

    title_match = re.match(r'^#\s+.*?\n', final_report)
    if title_match:
        # Replace existing title with our generated one
        final_report = re.sub(r'^#\s+.*?\n', f'# {report_title}\n', final_report, count=1)
    else:

        final_report = f'# {report_title}\n\n{final_report}'

    # Also check for second line being the full query, which happens sometimes
    lines = final_report.split('\n')
    if len(lines) > 2 and len(lines[1]) > 80 and "query" not in lines[1].lower():
        lines.pop(1)  # Remove the second line if it looks like a query
        final_report = '\n'.join(lines)

    if "References" in final_report:

        references_match = re.search(r'#+\s*References.*?(?=#+\s+|\Z)', final_report, re.DOTALL)
        if references_match:
            references_section = references_match.group(0)

            # Always replace the references section with our properly formatted web citations
            console.print("[yellow]Ensuring references are properly formatted as web citations...[/]")

            citation_registry = state.get("citation_registry")
            citation_manager = state.get("citation_manager")
            formatted_citations = ""

            if citation_manager and citation_registry:

                citation_stats = citation_manager.get_learning_statistics()
                console.print(f"[bold green]Report references {len(citation_registry.citations)} sources with {citation_stats.get('total_learnings', 0)} tracked learnings[/]")

                validation_result = citation_registry.validate_citations(final_report)

                if not validation_result["valid"]:

                    out_of_range_count = len(validation_result.get("out_of_range_citations", set()))
                    other_invalid_count = len(validation_result["invalid_citations"]) - out_of_range_count
                    max_valid_id = validation_result.get("max_valid_id", 0)

                    console.print(f"[bold yellow]Found {len(validation_result['invalid_citations'])} invalid citations in the report[/]")

                    if out_of_range_count > 0:
                        console.print(f"[bold red]Found {out_of_range_count} out-of-range citations (exceeding max valid ID: {max_valid_id})[/]")

                    # Remove invalid citations from the report
                    for invalid_cid in validation_result["invalid_citations"]:
                        # For out-of-range citations, replace with valid range indicator
                        if invalid_cid in validation_result.get("out_of_range_citations", set()):
                            replacement = f'[1-{max_valid_id}]'  # Suggest valid range
                            final_report = re.sub(f'\\[{invalid_cid}\\]', replacement, final_report)
                        else:
                            # Replace other invalid patterns like [invalid_cid] with [?]
                            final_report = re.sub(f'\\[{invalid_cid}\\]', '[?]', final_report)

                used_citations = validation_result["used_citations"]

                # If we have a citation manager, use its enhanced formatting
                if citation_manager and used_citations:

                    processed_text, bibliography_entries = citation_manager.get_citations_for_report(final_report)

                    # Use the citation manager's bibliography formatter with APA style
                    if bibliography_entries:
                        report_template = state.get('report_template', 'standard')
                        citation_style = "apa" # Default
                        if report_template == "academic":
                            citation_style = "apa"
                        elif report_template == "literature_review":
                            citation_style = "mla"
                        elif report_template == "business":
                            citation_style = "apa"

                        formatted_citations = citation_manager.format_bibliography(bibliography_entries, style=citation_style)
                        console.print(f"[bold green]Generated enhanced bibliography with {len(bibliography_entries)} entries using '{citation_style}' style.[/bold green]")
                # Fall back to regular citation formatting
                elif used_citations:

                    formatted_citations = await format_citations(
                        llm,
                        state.get('selected_sources', []),
                        state["sources"],
                        citation_registry
                    )

            # Replace references section with properly formatted ones
            if formatted_citations:
                new_references = f"# References\n\n{formatted_citations}\n"
                final_report = final_report.replace(references_section, new_references)
            elif state.get("formatted_citations"):
                new_references = f"# References\n\n{state['formatted_citations']}\n"
                final_report = final_report.replace(references_section, new_references)
            else:

                basic_references = []
                for i, url in enumerate(state.get("selected_sources", []), 1):
                    source_meta = next((s for s in state["sources"] if s.get("url") == url), {})
                    title = source_meta.get("title", "Untitled")
                    domain = url.split("//")[1].split("/")[0] if "//" in url else "Unknown Source"
                    date = source_meta.get("date", "n.d.")

                    # Simpler citation format without the date
                    citation = f"[{i}] *{domain}*, \"{title}\", {url}"
                    basic_references.append(citation)

                new_references = f"# References\n\n" + "\n".join(basic_references) + "\n"
                final_report = final_report.replace(references_section, new_references)

    elapsed_time = time.time() - state["start_time"]
    minutes, seconds = divmod(int(elapsed_time), 60)

    state["messages"].append(AIMessage(content="Research complete. Generating final report..."))
    state["findings"] = final_report
    state["status"] = "Complete"

    # Simulate chart execution and embed into final_report
    executed_charts_info_list = []
    chart_output_dir = "charts" # Relative directory for charts
    if not os.path.exists(chart_output_dir):
        os.makedirs(chart_output_dir)

    if "citation_manager" in state and state["citation_manager"]:
        citation_manager = state["citation_manager"]
        if hasattr(citation_manager, 'sources') and isinstance(citation_manager.sources, dict):
            console.print("[bold blue]Executing Matplotlib scripts and preparing for report embedding...[/]")
            for source_url, source_info in citation_manager.sources.items():
                if hasattr(source_info, 'visualizable_data') and isinstance(source_info.visualizable_data, list):
                    for data_item_idx, data_item in enumerate(source_info.visualizable_data):
                        if isinstance(data_item, dict) and \
                           data_item.get('matplotlib_code') and \
                           data_item.get('chart_filename'):

                            chart_code = data_item['matplotlib_code']
                            original_chart_filename = data_item['chart_filename']
                            chart_title = data_item.get('title_suggestion', f"Chart_{original_chart_filename}")

                            image_path = os.path.join(chart_output_dir, original_chart_filename)

                            # Modify plt.savefig() to use the absolute image_path
                            # Ensure image_path is properly escaped for use in a string literal if necessary,
                            # though os.path.join should produce a clean path.
                            # Python's string literals handle backslashes in paths correctly on Windows if they are raw or escaped.
                            # Forcing forward slashes for cross-platform consistency in the generated script:
                            safe_image_path_for_script = image_path.replace("\\", "/")
                            chart_code = re.sub(r"plt\.savefig\s*\(\s*['\"].*?['\"]\s*\)",
                                                f"plt.savefig(r'{safe_image_path_for_script}')",
                                                chart_code)
                            if "plt.savefig" not in chart_code: # If no savefig was present, add one.
                                chart_code += f"\nplt.savefig(r'{safe_image_path_for_script}')"

                            # Ensure plt.close() is in the script to free memory
                            if "plt.close()" not in chart_code:
                                chart_code += "\nplt.close()"

                            script_path = os.path.join(chart_output_dir, "temp_chart_script.py")
                            with open(script_path, "w") as f:
                                f.write(chart_code)

                            try:
                                # Consider using sys.executable for robustness
                                process = subprocess.run(["python", script_path], capture_output=True, text=True, timeout=30)
                                if process.returncode == 0:
                                    if os.path.exists(image_path) and os.path.getsize(image_path) > 0:
                                        console.print(f"[green]Successfully executed chart script and saved: {image_path}[/green]")
                                        executed_charts_info_list.append({
                                            'md_path': image_path.replace("\\", "/"), # Use forward slashes for MD
                                            'title': chart_title,
                                            'original_filename': original_chart_filename
                                        })
                                    else:
                                        console.print(f"[red]Chart script {original_chart_filename} ran but output file {image_path} not found or empty.[/red]")
                                        console.print(f"[red]STDOUT:\n{process.stdout}[/red]")
                                        console.print(f"[red]STDERR:\n{process.stderr}[/red]")
                                else:
                                    console.print(f"[red]Error executing chart script {original_chart_filename} (Return Code: {process.returncode}):[/red]")
                                    console.print(f"[red]STDOUT:\n{process.stdout}[/red]")
                                    console.print(f"[red]STDERR:\n{process.stderr}[/red]")
                            except subprocess.TimeoutExpired:
                                console.print(f"[red]Timeout executing chart script {original_chart_filename}[/red]")
                            except Exception as e:
                                console.print(f"[red]Failed to execute chart script {original_chart_filename}: {e}[/red]")
                                console.print(f"[dim]{traceback.format_exc()}[/dim]")
                            finally:
                                if os.path.exists(script_path):
                                    os.remove(script_path) # Clean up
                        else:
                            if isinstance(data_item, dict) and (not data_item.get('matplotlib_code') or not data_item.get('chart_filename')):
                                console.print(f"[yellow]Skipping chart execution for a visualizable_data item in {source_url} (idx: {data_item_idx}) due to missing 'matplotlib_code' or 'chart_filename'.[/yellow]")

    if executed_charts_info_list:
        charts_markdown_section = "\n\n## Visualizations\n\n"
        for chart_info in executed_charts_info_list:
            # Ensure title doesn't break Markdown image alt text or header
            safe_title = chart_info['title'].replace('"', "'").replace('\n', ' ')
            charts_markdown_section += f"### {safe_title}\n"
            charts_markdown_section += f"![{safe_title}]({chart_info['md_path']})\n\n"

        # Append the new charts section to the final report
        final_report += charts_markdown_section
        state["findings"] = final_report # Update state with the report including charts
        console.print(f"[green]Appended {len(executed_charts_info_list)} chart references to the final report.[/green]")


    if "citation_manager" in state:
        citation_stats = state["citation_manager"].get_learning_statistics()
        log_chain_of_thought(
            state,
            f"Generated final report after {minutes}m {seconds}s with {citation_stats.get('total_sources', 0)} sources and {citation_stats.get('total_learnings', 0)} tracked learnings, including {len(executed_charts_info_list)} chart simulations."
        )
    else:
        log_chain_of_thought(state, f"Generated final report after {minutes}m {seconds}s, including {len(executed_charts_info_list)} chart simulations.")

    if progress_callback:
        await _call_progress_callback(progress_callback, state)
    return state
