"""Report generation utilities with structured output."""
import os
from typing import List, Dict, Optional, Any, Union
import re
from datetime import datetime
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from shandu.prompts import get_system_prompt # Added import
from ..utils.citation_registry import CitationRegistry

# å­—æ•°æ§åˆ¶å’ŒéªŒè¯å‡½æ•°
def count_chinese_and_english_chars(text: str) -> int:
    """ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å­—ç¬¦çš„æ€»æ•°"""
    chinese_count = 0
    english_count = 0

    # å®šä¹‰ä¸­æ–‡å­—ç¬¦çš„UnicodeèŒƒå›´
    chinese_unicode_range = range(0x4E00, 0x9FFF + 1)

    for char in text:
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        if ord(char) in chinese_unicode_range:
            chinese_count += 1
        # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦
        elif char.isalpha():
            english_count += 1

    return chinese_count + english_count

def get_word_count_requirements(detail_level: str) -> Dict[str, int]:
    """æ ¹æ®è¯¦ç»†ç¨‹åº¦è·å–å­—æ•°è¦æ±‚"""
    if detail_level == "brief":
        return {
            "total_min": 4000,
            "total_target": 4000,
            "total_max": 5000,
            "main_section_min": 800,
            "sub_section_min": 300,
            "paragraph_min": 120
        }
    elif detail_level == "detailed":
        return {
            "total_min": 15000,
            "total_target": 15000,
            "total_max": 18000,
            "main_section_min": 2500,
            "sub_section_min": 1000,
            "paragraph_min": 150
        }
    elif detail_level.startswith("custom_"):
        try:
            word_count = int(detail_level.split('_')[1])
            return {
                "total_min": word_count,  # ä¸¥æ ¼è¦æ±‚è¾¾åˆ°ç›®æ ‡å­—æ•°
                "total_target": word_count,
                "total_max": int(word_count * 1.2),
                "main_section_min": max(word_count // 4, 1500),  # å¢åŠ ä¸»ç« èŠ‚æœ€å°å­—æ•°
                "sub_section_min": max(word_count // 8, 600),    # å¢åŠ å­ç« èŠ‚æœ€å°å­—æ•°
                "paragraph_min": max(word_count // 40, 150)      # å¢åŠ æ®µè½æœ€å°å­—æ•°
            }
        except (ValueError, IndexError):
            pass

    # Default to standard - å¤§å¹…æé«˜æ ‡å‡†è¦æ±‚
    return {
        "total_min": 15000,  # ã€å¢å¼ºã€‘æé«˜æœ€ä½å­—æ•°è¦æ±‚
        "total_target": 18000,  # ã€å¢å¼ºã€‘æé«˜ç›®æ ‡å­—æ•°
        "total_max": 22000,  # ã€å¢å¼ºã€‘æé«˜æœ€å¤§å­—æ•°
        "main_section_min": 3000,  # ã€å¢å¼ºã€‘æé«˜ä¸»ç« èŠ‚æœ€å°å­—æ•°
        "sub_section_min": 1200,  # ã€å¢å¼ºã€‘æé«˜å­ç« èŠ‚æœ€å°å­—æ•°
        "paragraph_min": 200  # ã€å¢å¼ºã€‘æé«˜æ®µè½æœ€å°å­—æ•°
    }

def analyze_report_structure(report_content: str) -> Dict[str, Any]:
    """åˆ†ææŠ¥å‘Šç»“æ„å’Œå­—æ•°åˆ†å¸ƒ"""
    # ä½¿ç”¨æ­£ç¡®çš„å­—æ•°ç»Ÿè®¡æ–¹æ³•
    total_chars = count_chinese_and_english_chars(report_content)

    # æå–ç« èŠ‚
    sections = []
    section_pattern = re.compile(r'(##\s+[^\n]+)(.*?)(?=##\s+|\Z)', re.DOTALL)
    section_matches = section_pattern.findall(report_content)

    for header, content in section_matches:
        section_chars = count_chinese_and_english_chars(content.strip())

        # æå–å­ç« èŠ‚
        subsections = []
        subsection_pattern = re.compile(r'(###\s+[^\n]+)(.*?)(?=###\s+|##\s+|\Z)', re.DOTALL)
        subsection_matches = subsection_pattern.findall(content)

        for sub_header, sub_content in subsection_matches:
            subsection_chars = count_chinese_and_english_chars(sub_content.strip())

            # ç»Ÿè®¡æ®µè½æ•°
            paragraphs = [p.strip() for p in sub_content.split('\n\n') if p.strip() and not p.strip().startswith('#')]
            paragraph_count = len(paragraphs)
            avg_paragraph_length = sum(count_chinese_and_english_chars(p) for p in paragraphs) / max(paragraph_count, 1)

            subsections.append({
                "header": sub_header.strip(),
                "char_count": subsection_chars,
                "paragraph_count": paragraph_count,
                "avg_paragraph_length": avg_paragraph_length
            })

        sections.append({
            "header": header.strip(),
            "char_count": section_chars,
            "subsections": subsections
        })

    return {
        "total_chars": total_chars,
        "sections": sections,
        "section_count": len(sections)
    }

def validate_report_quality(report_content: str, detail_level: str) -> Dict[str, Any]:
    """éªŒè¯æŠ¥å‘Šè´¨é‡æ˜¯å¦è¾¾åˆ°è¦æ±‚"""
    requirements = get_word_count_requirements(detail_level)
    analysis = analyze_report_structure(report_content)

    issues = []
    warnings = []

    # æ£€æŸ¥æ€»å­—æ•°ï¼ˆä½¿ç”¨å­—ç¬¦æ•°ï¼‰
    if analysis["total_chars"] < requirements["total_min"]:
        issues.append(f"æ€»å­—æ•°ä¸è¶³ï¼š{analysis['total_chars']} < {requirements['total_min']}")
    elif analysis["total_chars"] < requirements["total_target"]:
        warnings.append(f"æ€»å­—æ•°åå°‘ï¼š{analysis['total_chars']} < {requirements['total_target']}")

    # æ£€æŸ¥ç« èŠ‚è´¨é‡
    short_sections = []
    short_subsections = []
    one_sentence_subsections = []

    for section in analysis["sections"]:
        # è·³è¿‡å‚è€ƒæ–‡çŒ®ç­‰ç‰¹æ®Šç« èŠ‚
        if any(keyword in section["header"].lower() for keyword in ["å‚è€ƒæ–‡çŒ®", "references", "è‡´è°¢", "acknowledgments"]):
            continue

        if section["char_count"] < requirements["main_section_min"]:
            short_sections.append(f"{section['header']}: {section['char_count']}å­—")

        for subsection in section["subsections"]:
            if subsection["char_count"] < requirements["sub_section_min"]:
                short_subsections.append(f"{subsection['header']}: {subsection['char_count']}å­—")

            if subsection["paragraph_count"] <= 1:
                one_sentence_subsections.append(subsection["header"])

    if short_sections:
        issues.append(f"è¿‡çŸ­çš„ä¸»è¦ç« èŠ‚ ({len(short_sections)}ä¸ª): {', '.join(short_sections[:3])}")

    if short_subsections:
        issues.append(f"è¿‡çŸ­çš„å­ç« èŠ‚ ({len(short_subsections)}ä¸ª): {', '.join(short_subsections[:3])}")

    if one_sentence_subsections:
        issues.append(f"åªæœ‰ä¸€ä¸ªæ®µè½çš„å­ç« èŠ‚ ({len(one_sentence_subsections)}ä¸ª): {', '.join(one_sentence_subsections[:3])}")

    return {
        "is_valid": len(issues) == 0,
        "issues": issues,
        "warnings": warnings,
        "analysis": analysis,
        "requirements": requirements
    }

# Structured output models
class ReportTitle(BaseModel):
    """Structured output for report title generation."""
    title: str = Field(description="A concise, professional title for the research report")

class ThemeExtraction(BaseModel):
    """Structured output for theme extraction."""
    themes: List[str] = Field(description="List of key themes identified in the research")
    descriptions: List[str] = Field(description="Brief descriptions of each theme")

class InitialReport(BaseModel):
    """Structured output for initial report generation."""
    executive_summary: str = Field(description="Executive summary of the research findings")
    introduction: str = Field(description="Introduction to the research topic")
    sections: List[Dict[str, str]] = Field(description="List of report sections with titles and content")
    conclusion: str = Field(description="Conclusion summarizing the key findings")

class EnhancedReport(BaseModel):
    """Structured output for enhanced report generation."""
    enhanced_sections: List[Dict[str, str]] = Field(description="Enhanced sections with additional details")

class ExpandedSection(BaseModel):
    """Structured output for section expansion."""
    section_title: str = Field(description="Title of the section being expanded")
    expanded_content: str = Field(description="Expanded content for the section")

async def generate_title(llm: ChatOpenAI, query: str) -> str:
    """Generate a professional title for the report using structured output."""
    # Use structured output for title generation
    try:
        structured_llm = llm.with_structured_output(ReportTitle, method="json_mode")
    except Exception:
        structured_llm = llm.with_structured_output(ReportTitle, method="function_calling")

    # Use a direct system prompt without template variables
    system_prompt = """You are creating a professional, concise title for a research report.

    CRITICAL REQUIREMENTS - YOUR TITLE MUST:
    1. Be EXTREMELY CONCISE (8 words maximum)
    2. Be DESCRIPTIVE of the main topic
    3. Be PROFESSIONAL in tone
    4. NEVER start with words like "Evaluating", "Analyzing", "Assessment", "Study", "Investigation", etc.
    5. NEVER contain generic phrases like "A Comprehensive Overview", "An In-depth Look", etc.
    6. NEVER use the word "report", "research", "analysis", or similar meta-terms
    7. NEVER include prefixes like "Topic:", "Subject:", etc.
    8. NEVER be in question format
    9. NEVER be in sentence format - should be a noun phrase

    EXAMPLES OF GOOD TITLES:
    "NVIDIA RTX 5000 Series Gaming Performance"
    "Quantum Computing Applications in Cryptography"
    "Clean Energy Transition: Global Market Trends"
    """

    user_message = f"Create a professional, concise title (8 words max) for research about: {query}"

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", user_message)
    ])

    try:
        # Direct non-structured approach to avoid errors
        direct_prompt = f"""Create a professional, concise title (8 words max) for this research topic: {query}

        The title should be:
        - Extremely concise (8 words max)
        - Descriptive of the main topic
        - Professional in tone
        - NOT use words like "Evaluating", "Analyzing", "Assessment", "Report", etc.
        - NOT use generic phrases like "A Comprehensive Overview"
        - Be a noun phrase, not a question or sentence

        Return ONLY the title, nothing else. No quotation marks, no explanations.
        """

        # Use a direct, simplified approach
        simple_llm = llm.with_config({"temperature": 0.2})
        response = await simple_llm.ainvoke(direct_prompt)

        clean_title = response.content.replace("Title:", "").replace("\"", "").strip()
        return clean_title
    except Exception as e:
        print(f"Error in structured title generation: {str(e)}. Using simpler approach.")

        from ...utils.logger import log_error
        log_error("Error in structured title generation", e,
                 context=f"Query: {query}, Function: generate_title")
        # Fallback to non-structured approach
        simple_prompt = ChatPromptTemplate.from_messages([
            ("system", "Create a professional, concise title (8 words max) for a research report."),
            ("user", f"Topic: {query}")
        ])

        simple_llm = llm.with_config({"temperature": 0.2})
        simple_chain = simple_prompt | simple_llm
        title = await simple_chain.ainvoke({})

        clean_title = title.content.replace("Title: ", "").replace("\"", "").strip()

        return clean_title

async def extract_themes(llm: ChatOpenAI, findings: str) -> str:
    """Extract key themes from research findings using structured output."""
    # Use structured output for theme extraction
    try:
        structured_llm = llm.with_structured_output(ThemeExtraction, method="json_mode")
    except Exception:
        structured_llm = llm.with_structured_output(ThemeExtraction, method="function_calling")

    # Use a direct system prompt without template variables
    system_prompt = """You are analyzing research findings to identify key themes for a report structure.
    Extract 4-7 major themes (dont mention word Theme)from the content that would make logical report sections.
    These themes should emerge naturally from the content rather than following a predetermined structure.
    For each theme, provide a brief description of what content would be included."""

    user_message = f"Analyze these research findings and extract 4-7 key themes that should be used as main sections in a report:\n\n{findings}"

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", user_message)
    ])

    try:
        # Direct non-structured approach to avoid errors
        direct_prompt = f"""Extract 4-7 key themes from these research findings that would make logical report sections.

        Research findings:
        {findings[:3000]}

        Format your response as:

        ## Theme 1 (dont mention word Theme)
        Description of theme 1

        ## Theme 2 (dont mention word Theme)
        Description of theme 2

        And so on. Each theme should have a clear, concise title and a brief description.

        Do not include any other text or explanations outside of the themes and descriptions.
        """

        # Use direct approach
        simple_llm = llm.with_config({"temperature": 0.3})
        response = await simple_llm.ainvoke(direct_prompt)

        return response.content
    except Exception as e:
        print(f"Error in structured theme extraction: {str(e)}. Using simpler approach.")

        from ...utils.logger import log_error
        log_error("Error in structured theme extraction", e,
                 context=f"Findings length: {len(findings)}, Function: extract_themes")

        # Fallback to non-structured approach
        simple_prompt = ChatPromptTemplate.from_messages([
            ("system", """Extract 4-7 key themes from research findings that would make logical report sections.
            Format your response as:

            ## Theme 1 (dont mention word Theme)
            Description of theme 1

            ## Topic 2 (dont mention word Topic)
            Description of topic 2

            And so on."""),
            ("user", f"Extract topics from these findings:\n\n{findings[:10000]}")
        ])

        simple_llm = llm.with_config({"temperature": 0.3})
        simple_chain = simple_prompt | simple_llm
        themes = await simple_chain.ainvoke({})

        return themes.content

async def format_citations(
    llm: ChatOpenAI,
    selected_sources: List[str],
    sources: List[Dict[str, Any]],
    citation_registry: Optional[CitationRegistry] = None
) -> str:
    """
    Format citations for selected sources.

    Args:
        llm: The language model to use
        selected_sources: List of source URLs to format as citations
        sources: List of source metadata dictionaries
        citation_registry: Optional CitationRegistry instance to use for citation tracking

    Returns:
        String of formatted citations
    """
    if not selected_sources:
        return ""

    # If we have a citation registry, use it to get all properly cited sources
    if citation_registry:

        all_citations = citation_registry.get_all_citations()

        # Only format citations that were actually used
        # Sort by citation ID to ensure consistent ordering
        citations = []
        for cid in sorted(all_citations.keys()):
            citation_info = all_citations[cid]
            url = citation_info.get("url")

            source_meta = next((s for s in sources if s.get("url") == url), {})

            domain = url.split("//")[1].split("/")[0] if "//" in url else "Unknown Source"
            title = source_meta.get("title", citation_info.get("title", "Untitled"))
            date = source_meta.get("date", citation_info.get("date", "n.d."))

            citation = f"[{cid}] *{domain}*, \"{title}\", {url}"
            citations.append(citation)

        if citations:
            return "\n".join(citations)

    sources_text = ""
    for i, url in enumerate(selected_sources, 1):

        source_meta = {}
        for source in sources:
            if source.get("url") == url:
                source_meta = source
                break

        sources_text += f"Source {i}:\nURL: {url}\n"
        if source_meta.get("title"):
            sources_text += f"Title: {source_meta.get('title')}\n"
        if source_meta.get("source"):
            sources_text += f"Publication: {source_meta.get('source')}\n"
        if source_meta.get("date"):
            sources_text += f"Date: {source_meta.get('date')}\n"
        sources_text += "\n"

    # Use an enhanced citation formatter prompt with direct system message
    system_prompt = """Format the following source information into properly numbered citations for a research report.

    FORMATTING REQUIREMENTS:
    1. Each citation MUST start with [n] where n is the citation number
    2. Each citation MUST include the following elements (when available):
       - Website domain name in italics
       - Title of the article/page in quotes
       - Publication date
       - Complete URL

    EXAMPLE PROPER CITATIONS:
    [1] *techreview.com*, "Advances in GPU Architecture", 2024-01-15, https://techreview.com/articles/gpu-advances
    [2] *arxiv.org*, "Neural Network Performance Optimization", 2023-11-30, https://arxiv.org/papers/nn-optimization

    MISSING INFORMATION:
    - If website domain is missing, extract it from the URL
    - If title is missing, use "Untitled"
    - If date is missing, use "n.d." (no date)

    IMPORTANT: DO NOT generate citations from your training data. ONLY use the provided source information.
    DO NOT create academic-style citations like "Journal of Medicine (2020)".
    ONLY create web citations with the exact format shown in the examples.

    FORMAT ALL CITATIONS IN A CONSISTENT STYLE.
    Number citations sequentially starting from [1].
    Place each citation on a new line.
    """

    user_message = f"Format these sources into proper citations:\n\n{sources_text}"

    citation_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", user_message)
    ])

    formatter_chain = citation_prompt | llm
    formatted_citations = (await formatter_chain.ainvoke({})).content

    # Verify citations have proper format [n] at the beginning
    if not re.search(r'\[\d+\]', formatted_citations):

        citations = []
        for i, url in enumerate(selected_sources, 1):
            source_meta = next((s for s in sources if s.get("url") == url), {})
            title = source_meta.get("title", "Untitled")
            domain = url.split("//")[1].split("/")[0] if "//" in url else "Unknown Source"
            date = source_meta.get("date", "n.d.")

            citation = f"[{i}] *{domain}*, \"{title}\", {date}, {url}"
            citations.append(citation)

        formatted_citations = "\n".join(citations)

    return formatted_citations

async def generate_initial_report(
    llm: ChatOpenAI,
    query: str,
    findings: str,
    extracted_themes: str,
    report_title: str,
    selected_sources: List[str],
    formatted_citations: str,
    current_date: str,
    detail_level: str,
    include_objective: bool,
    citation_registry: Optional[CitationRegistry] = None,
    report_style_instructions: str = "", # Added
    language: str = "en", # Added
    length_instruction: str = "" # ã€æ–°å¢ã€‘å­—æ•°æ§åˆ¶æŒ‡ä»¤å‚æ•°
) -> str:
    """Generate the initial report draft using structured output."""

    # è·å–å­—æ•°è¦æ±‚
    requirements = get_word_count_requirements(detail_level)

    system_prompt_template = get_system_prompt("report_generation", language)

    # Variables to be injected into the system prompt template
    # Note: report_style_instructions is already localized by the caller
    # objective_instruction needs to be determined based on include_objective
    objective_instruction_text = ""
    if not include_objective:
        objective_instruction_text = "\n\nIMPORTANT: DO NOT include an \"Objective\" section at the beginning of the report. Let your content and analysis naturally determine the structure."

    # Prepare available sources text
    available_sources_text = ""
    if citation_registry:
        available_sources = []
        for cid in sorted(citation_registry.citations.keys()):
            citation_info = citation_registry.citations[cid]
            url = citation_info.get("url", "")
            title = citation_info.get("title", "") # Assuming title is stored
            available_sources.append(f"[{cid}] - {title} ({url})")
        if available_sources:
            available_sources_text = "\n\nAVAILABLE SOURCES FOR CITATION:\n" + "\n".join(available_sources)

    # Populate the system prompt using .format() for clarity with potentially complex fetched templates
    # The main template placeholders are {{current_date}}, {{report_style_instructions}}, {{objective_instruction}}
    # The system_prompt_template fetched from get_system_prompt should already contain these.
    # We also need to ensure detail_level is part of the system prompt if it's used there.
    # The original f-string was:
    # system_prompt = f"""You are generating a comprehensive research report...
    # - The level of detail should be {detail_level.upper()}.
    # - As of {current_date}, incorporate the most up-to-date information available.
    # - Create a dynamic structure based on the content themes rather than a rigid template.{objective_instruction}"""
    # The fetched template should ideally contain placeholders for these.
    # Let's assume the fetched `report_generation` prompt template has placeholders for:
    # {{current_date}}, {{report_style_instructions}}, {{objective_instruction}}, {{detail_level_placeholder}}

    # For now, I'll manually insert detail_level into the user message or assume it's part of report_style_instructions
    # or that the main system prompt template was designed to work without it explicitly.
    # The current `SYSTEM_PROMPTS["report_generation"]` does not have a placeholder for detail_level.
    # It's usually part of report_style_instructions or a general guideline.

    # The user message will carry the main content specifics
    if language.lower() == 'zh':
        user_message = f"""ğŸš¨ğŸš¨ğŸš¨ å¼ºåˆ¶æ€§æ·±åº¦å­¦æœ¯æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ - å­—æ•°éªŒè¯æ¨¡å¼ ğŸš¨ğŸš¨ğŸš¨

### ğŸ“‹ ç»å¯¹å¼ºåˆ¶è¦æ±‚ï¼ˆè¿èƒŒå°†å¯¼è‡´ä»»åŠ¡å¤±è´¥ï¼‰ï¼š
- **æ€»å­—æ•°è¦æ±‚**ï¼šå¿…é¡»ä¸¥æ ¼è¾¾åˆ° {requirements['total_target']} å­—ï¼ˆä¸­æ–‡å­—ç¬¦+è‹±æ–‡å­—ç¬¦æ€»æ•°ï¼‰
- **å­—æ•°éªŒè¯**ï¼šç”Ÿæˆåå°†è¿›è¡Œä¸¥æ ¼çš„å­—æ•°ç»Ÿè®¡éªŒè¯ï¼Œä¸è¾¾æ ‡å°†è¢«æ‹’ç»
- **å­¦æœ¯æ·±åº¦**ï¼šå¿…é¡»è¾¾åˆ°ç¡•å£«è®ºæ–‡æˆ–å­¦æœ¯æœŸåˆŠè´¨é‡æ°´å¹³
- **å†…å®¹å½¢å¼**ï¼šç»å¯¹ç¦æ­¢å¤§çº²å¼ã€è¦ç‚¹å¼å†…å®¹ï¼Œå¿…é¡»æ˜¯è¿è´¯çš„å­¦æœ¯æ–‡ç« 
- **æ®µè½è¦æ±‚**ï¼šæ¯ä¸ªæ®µè½è‡³å°‘200-300å­—ï¼ŒåŒ…å«å®Œæ•´çš„è®ºè¯è¿‡ç¨‹

{length_instruction}

### ğŸ“Š æŠ¥å‘Šè§„æ ¼ï¼š
æ ‡é¢˜ï¼š{report_title}
åŸå§‹ç ”ç©¶æŸ¥è¯¢ï¼š{query}
åˆ†æå‘ç°ï¼š{findings[:5000]}
æ¥æºæ•°é‡ï¼š{len(selected_sources)}
ç ”ç©¶ä¸­è¯†åˆ«çš„å…³é”®ä¸»é¢˜ï¼š
{extracted_themes}
{available_sources_text}

**é‡è¦æé†’**ï¼šæŠ¥å‘Šå†…å®¹å¿…é¡»ä¸¥æ ¼å›´ç»•åŸå§‹ç ”ç©¶æŸ¥è¯¢"{query}"å±•å¼€ï¼Œä¸å¾—åç¦»ä¸»é¢˜æˆ–æ··å…¥æ— å…³å†…å®¹ã€‚

### ğŸ¯ å¼ºåˆ¶æ€§å†…å®¹æ·±åº¦è¦æ±‚ï¼ˆæ¯é¡¹éƒ½å°†è¢«éªŒè¯ï¼‰ï¼š
1. **ç« èŠ‚å­—æ•°**ï¼šæ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘ {requirements['main_section_min']} å­—
2. **å­ç« èŠ‚å­—æ•°**ï¼šæ¯ä¸ªå­ç« èŠ‚è‡³å°‘ {requirements['sub_section_min']} å­—
3. **æ®µè½å¯†åº¦**ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘6-8ä¸ªå®Œæ•´æ®µè½
4. **è®ºè¯æ·±åº¦**ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½è¦ä»ç†è®ºåŸºç¡€ã€å†å²èƒŒæ™¯ã€ç°å®æ„ä¹‰ã€æœªæ¥å½±å“ã€å­¦æœ¯äº‰è®®äº”ä¸ªç»´åº¦å±•å¼€
5. **å…·ä½“æ¡ˆä¾‹**ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘2-3ä¸ªå…·ä½“æ¡ˆä¾‹æˆ–å®ä¾‹åˆ†æ

### ğŸ”¥ å†…å®¹è´¨é‡æ ‡å‡†ï¼ˆå°†è¢«ä¸¥æ ¼æ£€æŸ¥ï¼‰ï¼š
- **ç†è®ºæ·±åº¦**ï¼šæä¾›è¯¦å°½çš„ç†è®ºåˆ†æã€æ¦‚å¿µé˜é‡Šã€å­¦æœ¯èƒŒæ™¯ã€ç†è®ºå‘å±•è„‰ç»œ
- **å®è¯æ”¯æ’‘**ï¼šæ·»åŠ å…·ä½“æ¡ˆä¾‹ã€æ•°æ®åˆ†æã€å®è¯ç ”ç©¶ã€æƒå¨å¼•ç”¨ã€å­¦è€…è§‚ç‚¹
- **å­¦æœ¯è®ºè¯**ï¼šæ¯ä¸ªæ®µè½å¿…é¡»åŒ…å«ä¸»é¢˜å¥ã€è®ºæ®å±•å¼€ã€æ·±å…¥åˆ†æã€åæ€æ‰¹åˆ¤ã€å°ç»“è¿‡æ¸¡
- **è¿è´¯å™è¿°**ï¼šé‡‡ç”¨è¿è´¯çš„å­¦æœ¯å™è¿°é£æ ¼ï¼Œç»å¯¹ç¦æ­¢æ¡ç›®å¼ã€åˆ—è¡¨å¼è¡¨è¾¾
- **é€»è¾‘ä¸¥å¯†**ï¼šç¡®ä¿æ®µè½é—´ã€ç« èŠ‚é—´æœ‰æ¸…æ™°çš„é€»è¾‘è¿‡æ¸¡å’Œå†…åœ¨è”ç³»
- **æ·±åº¦åˆ†æ**ï¼šå¯¹æ¯ä¸ªè§‚ç‚¹è¿›è¡Œå¤šå±‚æ¬¡ã€å¤šè§’åº¦çš„æ·±å…¥å‰–æ

### âš ï¸ ç»å¯¹ç¦æ­¢ï¼ˆè¿åå°†å¯¼è‡´é‡æ–°ç”Ÿæˆï¼‰ï¼š
- å¤§çº²å¼å†…å®¹ï¼ˆå¦‚"- è¦ç‚¹1"ã€"â€¢ è¦ç‚¹2"ç­‰ï¼‰
- ç®€å•çš„è¦ç‚¹ç½—åˆ—
- è¿‡çŸ­çš„æ®µè½ï¼ˆå°‘äº200å­—ï¼‰
- ç¼ºä¹è®ºè¯çš„è§‚ç‚¹é™ˆè¿°
- æ¡†æ¶å¼æˆ–æçº²å¼å†…å®¹
- æµ…å±‚æ¬¡çš„åˆ†æå’Œè®¨è®º

### ğŸ“ å…·ä½“å†™ä½œæŒ‡å¯¼ï¼š
- æ¯ä¸ªæ®µè½å¼€å¤´è¦æœ‰æ˜ç¡®çš„ä¸»é¢˜å¥
- æ¯ä¸ªæ®µè½ä¸­é—´è¦æœ‰å……åˆ†çš„è®ºè¯å’Œåˆ†æ
- æ¯ä¸ªæ®µè½ç»“å°¾è¦æœ‰å°ç»“æˆ–è¿‡æ¸¡
- ä½¿ç”¨ä¸°å¯Œçš„å­¦æœ¯è¯æ±‡å’Œè¡¨è¾¾
- å¼•ç”¨å…·ä½“çš„ç ”ç©¶å‘ç°å’Œæ•°æ®
- æä¾›æ·±å…¥çš„ç†è®ºåˆ†æå’Œæ‰¹åˆ¤æ€è€ƒ

å›´ç»•è¿™äº›ä»ç ”ç©¶ä¸­è‡ªç„¶æ¶Œç°çš„å…³é”®ä¸»é¢˜ç»„ç»‡æ‚¨çš„æŠ¥å‘Šã€‚
åˆ›å»ºä¸€ä¸ªåŠ¨æ€ã€æœ‰æœºçš„ç»“æ„ï¼Œä»¥æœ€ä½³æ–¹å¼å‘ˆç°å‘ç°ï¼Œè€Œä¸æ˜¯å°†å†…å®¹å¼ºåˆ¶æ”¾å…¥é¢„å®šçš„ç« èŠ‚ä¸­ã€‚
ç¡®ä¿å…¨é¢è¦†ç›–ï¼ŒåŒæ—¶ä¿æŒä¸»é¢˜ä¹‹é—´çš„é€»è¾‘æµç¨‹ã€‚
è¯¦ç»†ç¨‹åº¦åº”ä¸º{detail_level.upper()}ã€‚

æ‚¨çš„æŠ¥å‘Šå¿…é¡»å¹¿æ³›ã€è¯¦ç»†ï¼Œå¹¶ä»¥ç ”ç©¶ä¸ºåŸºç¡€ã€‚åŒ…æ‹¬ç ”ç©¶ä¸­å‘ç°çš„æ‰€æœ‰ç›¸å…³æ•°æ®ã€ç¤ºä¾‹å’Œè§è§£ã€‚
åœ¨æ•´ä¸ªæŠ¥å‘Šä¸­ä½¿ç”¨é€‚å½“çš„å¼•æ–‡ï¼Œä»…å¼•ç”¨ä¸Šé¢åˆ—å‡ºçš„å¯ç”¨æ¥æºã€‚

å…³é”®è¦æ±‚ï¼šæ‚¨å¿…é¡»ä¸¥æ ¼éµå¾ªç³»ç»Ÿæç¤ºä¸­æä¾›çš„æ‰€æœ‰`report_style_instructions`æŒ‡ä»¤ï¼ŒåŒ…æ‹¬æ‰€é€‰æŠ¥å‘Šæ ·å¼ç‰¹å®šçš„ç»“æ„ã€è¯­è°ƒå’Œå†…å®¹è¦æ±‚ï¼š{report_style_instructions}

ğŸ¯ **æœ€ç»ˆæ‰§è¡ŒæŒ‡ä»¤**ï¼šè¯·ç«‹å³ç”Ÿæˆä¸€ä»½ä¸¥æ ¼è¾¾åˆ° {requirements['total_target']} å­—çš„æ·±åº¦å­¦æœ¯ç ”ç©¶æŠ¥å‘Šã€‚æ¯ä¸ªç« èŠ‚ã€æ¯ä¸ªæ®µè½éƒ½å¿…é¡»æœ‰å……åˆ†çš„å†…å®¹æ·±åº¦å’Œå­¦æœ¯è´¨é‡ã€‚ç”Ÿæˆåå°†è¿›è¡Œå­—æ•°éªŒè¯ï¼Œä¸è¾¾æ ‡å°†è¢«è¦æ±‚é‡æ–°ç”Ÿæˆã€‚

é‡è¦æç¤ºï¼šä»¥æä¾›çš„ç¡®åˆ‡æ ‡é¢˜å¼€å§‹æ‚¨çš„æŠ¥å‘Šï¼š"{report_title}" - ä¸è¦ä¿®æ”¹æˆ–é‡æ–°è¡¨è¿°å®ƒã€‚"""
    else:
        user_message = f"""ğŸš¨ğŸš¨ MANDATORY DEEP ACADEMIC REPORT GENERATION TASK ğŸš¨ğŸš¨

### ğŸ“‹ CORE REQUIREMENTS (ABSOLUTELY NON-NEGOTIABLE):
- **Total Word Count**: Must strictly reach {requirements['total_target']} words
- **Academic Depth**: Must meet master's thesis or academic journal quality standards
- **Content Form**: Absolutely NO outline-style or bullet-point content, must be coherent academic prose
- **Paragraph Requirements**: Each paragraph minimum 150-250 words with complete argumentation

{length_instruction}

### ğŸ“Š REPORT SPECIFICATIONS:
Title: {report_title}
Analyzed Findings: {findings[:5000]}
Number of sources: {len(selected_sources)}
Key themes identified in the research:
{extracted_themes}
{available_sources_text}

### ğŸ¯ MANDATORY CONTENT DEPTH REQUIREMENTS:
1. **Section Word Count**: Each major section minimum {requirements['main_section_min']} words
2. **Subsection Word Count**: Each subsection minimum {requirements['sub_section_min']} words
3. **Paragraph Density**: Each subsection must contain at least 4-6 complete paragraphs
4. **Argumentation Depth**: Each point must be developed from theoretical foundation, historical background, current significance, and future implications

### ğŸ”¥ CONTENT QUALITY STANDARDS:
- **Theoretical Depth**: Provide detailed theoretical analysis, concept explanation, academic background
- **Empirical Support**: Add specific cases, data analysis, empirical research and authoritative citations
- **Academic Argumentation**: Each paragraph must include topic sentence, evidence development, analytical interpretation, transitional conclusion
- **Coherent Narrative**: Use coherent academic narrative style, absolutely NO itemized or list-style expression
- **Logical Rigor**: Ensure clear logical transitions and internal connections between paragraphs and sections

### âš ï¸ ABSOLUTELY FORBIDDEN:
- Outline-style content (like "- Point 1", "â€¢ Point 2", etc.)
- Simple bullet point lists
- Overly short paragraphs (less than 150 words)
- Unsupported opinion statements
- Framework or outline-style content

Organize your report around these key themes that naturally emerged from the research.
Create a dynamic, organic structure that best presents the findings, rather than forcing content into predetermined sections.
Ensure comprehensive coverage while maintaining a logical flow between topics.
The level of detail should be {detail_level.upper()}.

Your report must be extensive, detailed, and grounded in the research. Include all relevant data, examples, and insights found in the research.
Use proper citations to the sources throughout, referring only to the available sources listed above.

It is CRITICAL that you strictly follow all directives within the `report_style_instructions` provided in the system prompt, including structural, tonal, and content requirements specific to the chosen report style: {report_style_instructions}

ğŸ¯ **EXECUTION COMMAND**: Immediately generate a {requirements['total_target']}-word deep academic research report ensuring each section has sufficient content depth and academic quality.

IMPORTANT: Begin your report with the exact title provided: "{report_title}" - do not modify or rephrase it."""

    # Create the prompt using the localized system template
    # The system prompt itself will be formatted by LangChain using these values.
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt_template), # Localized base template
        ("user", user_message)
    ])

    # Context for LangChain's .ainvoke method if system prompt has {{placeholders}}
    prompt_context = {
        "current_date": current_date,
        "report_style_instructions": report_style_instructions, # Already localized
        "objective_instruction": objective_instruction_text,
        # Add other placeholders if the fetched system_prompt_template uses them
    }

    try:
        # Fallback direct_prompt also needs localization.
        # For simplicity, we might use a generic instruction here or a simplified localized prompt.
        # Let's assume the main path with ChatPromptTemplate works.
        # The original direct_prompt was an f-string. If we want to localize this,
        # it also needs to be fetched. For now, let's focus on the main path.
        # If direct_prompt is still needed, its template should be fetched via get_system_prompt.
        # Simplified direct_prompt for fallback:
        direct_prompt_template_key = "direct_initial_report_generation" # Needs to be in prompts.py
        direct_prompt_base_template = get_system_prompt(direct_prompt_template_key, language)
        if not direct_prompt_base_template: # Fallback if key not found
            if language == "zh":
                direct_prompt_base_template = f"""ğŸš¨ğŸš¨ å¼ºåˆ¶æ€§æ·±åº¦å­¦æœ¯æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ ğŸš¨ğŸš¨

### ğŸ“‹ æ ¸å¿ƒè¦æ±‚ï¼ˆç»å¯¹ä¸å¯è¿èƒŒï¼‰ï¼š
- **æ€»å­—æ•°è¦æ±‚**ï¼šå¿…é¡»ä¸¥æ ¼è¾¾åˆ° {requirements['total_target']} å­—
- **å­¦æœ¯æ·±åº¦**ï¼šå¿…é¡»è¾¾åˆ°ç¡•å£«è®ºæ–‡æˆ–å­¦æœ¯æœŸåˆŠè´¨é‡æ°´å¹³
- **å†…å®¹å½¢å¼**ï¼šç»å¯¹ç¦æ­¢å¤§çº²å¼ã€è¦ç‚¹å¼å†…å®¹ï¼Œå¿…é¡»æ˜¯è¿è´¯çš„å­¦æœ¯æ–‡ç« 
- **æ®µè½è¦æ±‚**ï¼šæ¯ä¸ªæ®µè½è‡³å°‘150-250å­—ï¼ŒåŒ…å«å®Œæ•´çš„è®ºè¯è¿‡ç¨‹

### ğŸ“Š æŠ¥å‘Šè§„æ ¼ï¼š
æ ‡é¢˜ï¼š{{report_title}}
åŸºäºç ”ç©¶å‘ç°ï¼š{{findings_preview}}
ä¸»é¢˜ç»“æ„ï¼š{{themes}}
å¯ç”¨æ¥æºï¼š{{sources_info}}
è¯¦ç»†ç¨‹åº¦ï¼š{{detail_level}}
å½“å‰æ—¥æœŸï¼š{{current_date}}

### ğŸ¯ å¼ºåˆ¶æ€§å†…å®¹è¦æ±‚ï¼š
1. **ç« èŠ‚å­—æ•°**ï¼šæ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘ {requirements['main_section_min']} å­—
2. **å­ç« èŠ‚å­—æ•°**ï¼šæ¯ä¸ªå­ç« èŠ‚è‡³å°‘ {requirements['sub_section_min']} å­—
3. **æ®µè½å¯†åº¦**ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘4-6ä¸ªå®Œæ•´æ®µè½
4. **è®ºè¯æ·±åº¦**ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½è¦ä»ç†è®ºåŸºç¡€ã€å†å²èƒŒæ™¯ã€ç°å®æ„ä¹‰ã€æœªæ¥å½±å“å››ä¸ªç»´åº¦å±•å¼€

### ğŸ”¥ å†…å®¹è´¨é‡æ ‡å‡†ï¼š
- **ç†è®ºæ·±åº¦**ï¼šæä¾›è¯¦å°½çš„ç†è®ºåˆ†æã€æ¦‚å¿µé˜é‡Šã€å­¦æœ¯èƒŒæ™¯
- **å®è¯æ”¯æ’‘**ï¼šæ·»åŠ å…·ä½“æ¡ˆä¾‹ã€æ•°æ®åˆ†æã€å®è¯ç ”ç©¶å’Œæƒå¨å¼•ç”¨
- **å­¦æœ¯è®ºè¯**ï¼šæ¯ä¸ªæ®µè½å¿…é¡»åŒ…å«ä¸»é¢˜å¥ã€è®ºæ®å±•å¼€ã€åˆ†æé˜é‡Šã€å°ç»“è¿‡æ¸¡
- **è¿è´¯å™è¿°**ï¼šé‡‡ç”¨è¿è´¯çš„å­¦æœ¯å™è¿°é£æ ¼ï¼Œç»å¯¹ç¦æ­¢æ¡ç›®å¼ã€åˆ—è¡¨å¼è¡¨è¾¾
- **é€»è¾‘ä¸¥å¯†**ï¼šç¡®ä¿æ®µè½é—´ã€ç« èŠ‚é—´æœ‰æ¸…æ™°çš„é€»è¾‘è¿‡æ¸¡å’Œå†…åœ¨è”ç³»

### ğŸ“ æ ¼å¼è¦æ±‚ï¼š
- éµå¾ªMarkdownæ ¼å¼
- åŒ…å«æ ‡é¢˜ã€æ‘˜è¦ã€å¼•è¨€ã€æŒ‰ä¸»é¢˜åˆ†ç« èŠ‚ã€è®¨è®ºä¸åˆ†æã€ç»“è®ºå’Œå¸¦æ­£ç¡®å¼•ç”¨[n]çš„å‚è€ƒæ–‡çŒ®
- æ¯ä¸ªç« èŠ‚éƒ½è¦æœ‰è¯¦ç»†çš„å­ç« èŠ‚åˆ’åˆ†
- ç¡®ä¿å¼•ç”¨æ ¼å¼æ­£ç¡®ï¼Œä½¿ç”¨[n]æ ¼å¼

### âš ï¸ ç»å¯¹ç¦æ­¢ï¼š
- å¤§çº²å¼å†…å®¹ï¼ˆå¦‚"- è¦ç‚¹1"ã€"â€¢ è¦ç‚¹2"ç­‰ï¼‰
- ç®€å•çš„è¦ç‚¹ç½—åˆ—
- è¿‡çŸ­çš„æ®µè½ï¼ˆå°‘äº150å­—ï¼‰
- ç¼ºä¹è®ºè¯çš„è§‚ç‚¹é™ˆè¿°
- æ¡†æ¶å¼æˆ–æçº²å¼å†…å®¹

ğŸ¯ **æ‰§è¡ŒæŒ‡ä»¤**ï¼šè¯·ç«‹å³ç”Ÿæˆä¸€ä»½è¾¾åˆ° {requirements['total_target']} å­—çš„æ·±åº¦å­¦æœ¯ç ”ç©¶æŠ¥å‘Šï¼Œç¡®ä¿æ¯ä¸ªç« èŠ‚éƒ½æœ‰å……åˆ†çš„å†…å®¹æ·±åº¦å’Œå­¦æœ¯è´¨é‡ã€‚"""
            else:
                direct_prompt_base_template = f"""ğŸš¨ğŸš¨ MANDATORY DEEP ACADEMIC REPORT GENERATION TASK ğŸš¨ğŸš¨

### ğŸ“‹ CORE REQUIREMENTS (ABSOLUTELY NON-NEGOTIABLE):
- **Total Word Count**: Must strictly reach {requirements['total_target']} words
- **Academic Depth**: Must meet master's thesis or academic journal quality standards
- **Content Form**: Absolutely NO outline-style or bullet-point content, must be coherent academic prose
- **Paragraph Requirements**: Each paragraph minimum 150-250 words with complete argumentation

### ğŸ“Š REPORT SPECIFICATIONS:
Title: {{report_title}}
Based on findings: {{findings_preview}}
Theme structure: {{themes}}
Available sources: {{sources_info}}
Detail level: {{detail_level}}
Current date: {{current_date}}

### ğŸ¯ MANDATORY CONTENT REQUIREMENTS:
1. **Section Word Count**: Each major section minimum {requirements['main_section_min']} words
2. **Subsection Word Count**: Each subsection minimum {requirements['sub_section_min']} words
3. **Paragraph Density**: Each subsection must contain at least 4-6 complete paragraphs
4. **Argumentation Depth**: Each point must be developed from theoretical foundation, historical background, current significance, and future implications

### ğŸ”¥ CONTENT QUALITY STANDARDS:
- **Theoretical Depth**: Provide detailed theoretical analysis, concept explanation, academic background
- **Empirical Support**: Add specific cases, data analysis, empirical research and authoritative citations
- **Academic Argumentation**: Each paragraph must include topic sentence, evidence development, analytical interpretation, transitional conclusion
- **Coherent Narrative**: Use coherent academic narrative style, absolutely NO itemized or list-style expression
- **Logical Rigor**: Ensure clear logical transitions and internal connections between paragraphs and sections

### ğŸ“ FORMAT REQUIREMENTS:
- Follow Markdown format
- Include title, abstract, introduction, thematic sections, discussion & analysis, conclusion and references with correct [n] citations
- Each section must have detailed subsection divisions
- Ensure correct citation format using [n] format

### âš ï¸ ABSOLUTELY FORBIDDEN:
- Outline-style content (like "- Point 1", "â€¢ Point 2", etc.)
- Simple bullet point lists
- Overly short paragraphs (less than 150 words)
- Unsupported opinion statements
- Framework or outline-style content

ğŸ¯ **EXECUTION COMMAND**: Immediately generate a {requirements['total_target']}-word deep academic research report ensuring each section has sufficient content depth and academic quality."""

        direct_prompt_content = direct_prompt_base_template.format(
            report_title=report_title,
            findings_preview=findings[:3000], # å¢åŠ é¢„è§ˆå†…å®¹ä»¥æä¾›æ›´å¤šä¿¡æ¯
            themes=extracted_themes,
            sources_info=available_sources_text,
            detail_level=detail_level.upper(),
            current_date=current_date,
            report_style_instructions=report_style_instructions,
            length_instruction=length_instruction  # ã€ä¿®å¤ã€‘æ·»åŠ ç¼ºå¤±çš„length_instructionå‚æ•°
        )

        report_llm = llm.with_config({"max_tokens": 120000, "temperature": 0.5})  # ã€å¢å¼ºã€‘æé«˜max_tokenså’Œæ¸©åº¦
        # Invoke with the structured prompt first
        # response = await (prompt | report_llm).ainvoke(prompt_context) # This would be the ideal structured way

        # The original code uses a direct invoke on a manually constructed "direct_prompt_content" for the main path.
        # This "direct_prompt_content" should be constructed from a localized template.
        # The `report_generation` system prompt is quite complex.
        # Let's use the fetched system_prompt_template for the main call if possible,
        # but the user message is very specific.
        # Re-constructing the main path to use the fetched system prompt with ChatPromptTemplate:

        final_system_prompt = system_prompt_template.format(
            current_date=current_date,
            report_style_instructions=report_style_instructions,
            objective_instruction=objective_instruction_text,
            # detail_level is in user_message for now
        )

        final_prompt_messages = [
            ("system", final_system_prompt),
            ("user", user_message)
        ]

        response = await report_llm.ainvoke(final_prompt_messages)
        initial_report = response.content

        # éªŒè¯æŠ¥å‘Šè´¨é‡å¹¶è¿›è¡Œé‡è¯•
        max_retries = 3
        for attempt in range(max_retries):
            validation = validate_report_quality(initial_report, detail_level)

            if validation["is_valid"]:
                print(f"âœ… æŠ¥å‘Šè´¨é‡éªŒè¯é€šè¿‡ (æ€»å­—æ•°: {validation['analysis']['total_chars']})")
                return initial_report

            print(f"âš ï¸ æŠ¥å‘Šè´¨é‡éªŒè¯å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries})")
            for issue in validation["issues"]:
                print(f"   - {issue}")

            if attempt < max_retries - 1:
                # æ„å»ºå¼ºåˆ¶æ€§æ”¹è¿›æç¤º
                current_chars = validation['analysis']['total_chars']
                needed_chars = validation['requirements']['total_target'] - current_chars

                improvement_prompt = f"""ğŸš¨ğŸš¨ å¼ºåˆ¶æ€§æŠ¥å‘Šä¿®å¤ä»»åŠ¡ ğŸš¨ğŸš¨

**å½“å‰é—®é¢˜è¯Šæ–­ï¼š**
- å½“å‰å­—æ•°ï¼š{current_chars}å­—ï¼ˆä¸¥é‡ä¸è¶³ï¼‰
- ç›®æ ‡å­—æ•°ï¼š{validation['requirements']['total_target']}å­—
- éœ€è¦å¢åŠ ï¼š{needed_chars}å­—

**å…·ä½“é—®é¢˜ï¼š**
{chr(10).join(f"- {issue}" for issue in validation["issues"])}

**åŸå§‹æŠ¥å‘Šï¼š**
{initial_report}

### ğŸš¨ å¼ºåˆ¶æ€§ä¿®å¤è¦æ±‚ï¼ˆç»å¯¹ä¸å¯è¿èƒŒï¼‰ï¼š

#### 1. å­—æ•°å¼ºåˆ¶è¦æ±‚
- **æ€»å­—æ•°**ï¼šå¿…é¡»è¾¾åˆ°{validation['requirements']['total_target']}å­—
- **ä¸»ç« èŠ‚**ï¼šæ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘{validation['requirements']['main_section_min']}å­—
- **å­ç« èŠ‚**ï¼šæ¯ä¸ªå­ç« èŠ‚è‡³å°‘{validation['requirements']['sub_section_min']}å­—

#### 2. å†…å®¹æ·±åº¦è¦æ±‚
- **æ®µè½å¯†åº¦**ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘4-6ä¸ªå®Œæ•´æ®µè½
- **æ®µè½é•¿åº¦**ï¼šæ¯ä¸ªæ®µè½è‡³å°‘150-250å­—ï¼ŒåŒ…å«å®Œæ•´çš„è®ºè¯è¿‡ç¨‹
- **å­¦æœ¯æ·±åº¦**ï¼šå¿…é¡»è¾¾åˆ°ç¡•å£«è®ºæ–‡æˆ–å­¦æœ¯æœŸåˆŠè´¨é‡æ°´å¹³

#### 3. ç»“æ„å®Œæ•´æ€§
- **æ¶ˆé™¤ç©ºç« èŠ‚**ï¼šæ‰€æœ‰ç« èŠ‚éƒ½å¿…é¡»æœ‰å®è´¨æ€§å†…å®¹
- **é€»è¾‘è¿è´¯**ï¼šç¡®ä¿ç« èŠ‚é—´æœ‰æ¸…æ™°çš„é€»è¾‘è¿‡æ¸¡
- **å†…å®¹å……å®**ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½è¦æä¾›è¯¦ç»†çš„ç†è®ºåˆ†æå’Œå®è¯æ”¯æ’‘

#### 4. è´¨é‡æ ‡å‡†
- **ç»å¯¹ç¦æ­¢**ï¼šå¤§çº²å¼ã€è¦ç‚¹å¼å†…å®¹
- **å¿…é¡»é‡‡ç”¨**ï¼šè¿è´¯çš„å­¦æœ¯å™è¿°é£æ ¼
- **æ·±åº¦è®ºè¯**ï¼šæ¯ä¸ªè§‚ç‚¹ä»ç†è®ºåŸºç¡€ã€å†å²èƒŒæ™¯ã€ç°å®æ„ä¹‰ã€æœªæ¥å½±å“å››ä¸ªç»´åº¦å±•å¼€

ğŸ¯ **æ‰§è¡ŒæŒ‡ä»¤**ï¼šè¯·ç«‹å³ç”Ÿæˆä¸€ä»½å®Œå…¨ç¬¦åˆä¸Šè¿°è¦æ±‚çš„{validation['requirements']['total_target']}å­—æ·±åº¦å­¦æœ¯ç ”ç©¶æŠ¥å‘Šã€‚"""

                retry_llm = llm.with_config({"max_tokens": 120000, "temperature": 0.4})  # ã€å¢å¼ºã€‘æé«˜é‡è¯•æ—¶çš„max_tokens
                retry_response = await retry_llm.ainvoke(improvement_prompt)
                initial_report = retry_response.content

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿›è¡Œå¼ºåˆ¶æ€§å­—æ•°æ‰©å±•
        print(f"âš ï¸ ç»è¿‡ {max_retries} æ¬¡é‡è¯•ï¼ŒæŠ¥å‘Šä»æœªå®Œå…¨è¾¾åˆ°è¦æ±‚ï¼Œå¼€å§‹å¼ºåˆ¶æ€§æ‰©å±•...")

        # ã€æ–°å¢ã€‘å¼ºåˆ¶æ€§å¤šè½®æ‰©å±•æœºåˆ¶
        try:
            final_expanded_report = await force_word_count_compliance(
                llm, initial_report, detail_level, language
            )

            # æœ€ç»ˆéªŒè¯
            final_validation = validate_report_quality(final_expanded_report, detail_level)
            if final_validation["is_valid"]:
                print(f"âœ… å¼ºåˆ¶æ‰©å±•æˆåŠŸï¼ŒæŠ¥å‘Šè´¨é‡è¾¾æ ‡ (æ€»å­—æ•°: {final_validation['analysis']['total_chars']})")
                return final_expanded_report
            else:
                print(f"âš ï¸ å¼ºåˆ¶æ‰©å±•åä»æœ‰é—®é¢˜ï¼Œä½†å­—æ•°å·²æ”¹å–„ (æ€»å­—æ•°: {final_validation['analysis']['total_chars']})")
                return final_expanded_report
        except Exception as e:
            print(f"âŒ å¼ºåˆ¶æ‰©å±•å¤±è´¥: {e}")
            return initial_report

    except Exception as e:
        print(f"Error in structured report generation: {str(e)}. Using simpler approach.")
        from ...utils.logger import log_error
        log_error("Error in structured report generation", e,
                 context={"query": query, "report_title": report_title, "language": language, "function": "generate_initial_report"})

        # Fallback to simpler report generation
        simple_system_template = get_system_prompt("simple_report_fallback", language) # New key needed
        if not simple_system_template:
            simple_system_template = "Generate a research report based on findings. Date: {current_date}."

        simple_system_message = simple_system_template.format(current_date=current_date)

        if language.lower() == 'zh':
            simple_user_message = f"æ ‡é¢˜ï¼š{report_title}\n\nå‘ç°ï¼ˆé¢„è§ˆï¼‰ï¼š{findings[:5000]}\n{available_sources_text}"
        else:
            simple_user_message = f"Title: {report_title}\n\nFindings (preview): {findings[:5000]}\n{available_sources_text}"

        simple_prompt = ChatPromptTemplate.from_messages([
            ("system", simple_system_message),
            ("user", simple_user_message)
        ])

        # æ ¹æ®å­—æ•°è¦æ±‚è°ƒæ•´max_tokens
        required_chars = requirements['total_target']
        # ä¼°ç®—éœ€è¦çš„tokensï¼ˆä¸­æ–‡å­—ç¬¦çº¦1.5å€ï¼Œè‹±æ–‡å­—ç¬¦çº¦0.25å€ï¼‰
        estimated_tokens = int(required_chars * 1.8)  # ä¿å®ˆä¼°è®¡
        max_tokens = min(estimated_tokens, 120000)  # ä¸è¶…è¿‡æ¨¡å‹é™åˆ¶

        simple_llm = llm.with_config({"max_tokens": max_tokens, "temperature": 0.6})
        response = await (simple_prompt | simple_llm).ainvoke({})

        # éªŒè¯ç”Ÿæˆçš„å†…å®¹å­—æ•°
        generated_content = response.content
        actual_chars = count_chinese_and_english_chars(generated_content)

        # å¦‚æœå­—æ•°ä¸¥é‡ä¸è¶³ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ
        if actual_chars < requirements['total_min'] * 0.7:  # å°‘äºæœ€ä½è¦æ±‚çš„70%
            print(f"åˆæ¬¡ç”Ÿæˆå­—æ•°ä¸è¶³({actual_chars}å­—)ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ...")

            # ä½¿ç”¨æ›´å¼ºçš„æç¤ºè¯é‡æ–°ç”Ÿæˆ
            enhanced_prompt = f"""ğŸš¨ğŸš¨ğŸš¨ ç´§æ€¥å­—æ•°è¦æ±‚ ğŸš¨ğŸš¨ğŸš¨

å½“å‰ç”Ÿæˆçš„å†…å®¹åªæœ‰{actual_chars}å­—ï¼Œä¸¥é‡ä¸è¶³ï¼

å¿…é¡»é‡æ–°ç”Ÿæˆä¸€ä»½è‡³å°‘{requirements['total_target']}å­—çš„å®Œæ•´æŠ¥å‘Šã€‚

{user_message}

âš ï¸ ç‰¹åˆ«æ³¨æ„ï¼š
- æ¯ä¸ªæ®µè½å¿…é¡»è‡³å°‘300å­—
- æ¯ä¸ªå­ç« èŠ‚å¿…é¡»è‡³å°‘800å­—
- æ¯ä¸ªä¸»è¦ç« èŠ‚å¿…é¡»è‡³å°‘2000å­—
- ç»å¯¹ä¸èƒ½æ˜¯å¤§çº²å¼å†…å®¹
- å¿…é¡»æ˜¯è¿è´¯çš„å­¦æœ¯æ–‡ç« 

è¯·ç«‹å³é‡æ–°ç”Ÿæˆå®Œæ•´çš„{requirements['total_target']}å­—æŠ¥å‘Šï¼"""

            enhanced_llm = llm.with_config({"max_tokens": max_tokens, "temperature": 0.7})
            enhanced_response = await enhanced_llm.ainvoke(enhanced_prompt)
            generated_content = enhanced_response.content
            actual_chars = count_chinese_and_english_chars(generated_content)
            print(f"é‡æ–°ç”Ÿæˆåå­—æ•°: {actual_chars}å­—")

        return generated_content

async def enhance_report(
    llm: ChatOpenAI,
    initial_report: str,
    current_date: str,
    # formatted_citations: str, # Not directly used in prompt construction here
    # selected_sources: List,   # Not directly used
    # sources: List[Dict],      # Not directly used
    citation_registry: Optional[CitationRegistry] = None,
    report_style_instructions: str = "",
    language: str = "en",
    length_instruction: str = "" # Added
) -> str:
    """Enhance the report with additional detail while preserving structure and providing context."""

    if not initial_report or len(initial_report.strip()) < 500:
        return initial_report

    title_match = re.match(r'# ([^\n]+)', initial_report)
    report_title = title_match.group(1) if title_match else "Research Report"

    # Store sections as list of dicts to easily access header and content
    raw_sections = re.compile(r'(#+\s+[^\n]+)((?:\n\n[^#]+?)*)(?=\n#+\s+|\Z)', re.DOTALL).findall(initial_report)
    sections_data = [{'header': header, 'content': content.strip()} for header, content in raw_sections if header and content.strip()]

    if not sections_data:
        return initial_report

    # Determine report_summary_context
    report_summary_context = "Summary not available."
    if sections_data:
        first_section_title = sections_data[0]['header'].lower().replace('#', '').strip()
        if "introduction" in first_section_title or "executive summary" in first_section_title:
            report_summary_context = sections_data[0]['content'][:500] + "..." if len(sections_data[0]['content']) > 500 else sections_data[0]['content']
        else: # Fallback to first few paras of the report if no clear intro/summary
            intro_text_match = re.search(r'#\s*[^\n]+\n+([^#]+)', initial_report, re.DOTALL)
            if intro_text_match:
                report_summary_context = intro_text_match.group(1).strip()[:500] + "..." if len(intro_text_match.group(1).strip()) > 500 else intro_text_match.group(1).strip()


    enhanced_report_parts = [f"# {report_title}\n\n"]

    base_section_prompt_template = get_system_prompt("enhance_section_detail_template", language)
    if not base_section_prompt_template:
        base_section_prompt_template = """Enhance this section of a research report with additional depth and detail:

{section_header_content}{available_sources_text}

Your task is to:
1. Add more detailed explanations to key concepts
2. Expand on examples and case studies
3. Enhance the analysis and interpretation of findings
4. Improve the flow within this section
5. Add relevant statistics, data points, or evidence
6. Ensure proper citation [n] format throughout
7. Maintain scientific accuracy and up-to-date information (current as of {current_date})

CITATION REQUIREMENTS:
- ONLY use the citation IDs provided in the AVAILABLE SOURCES list above
- Format citations as [n] where n is the exact ID of the source
- Place citations at the end of the relevant sentences or paragraphs
- Do not make up your own citation numbers
- Do not cite sources that aren't in the available sources list

IMPORTANT:
- DO NOT change the section heading
- DO NOT add information not supported by the research
- DO NOT use academic-style citations like "Journal of Medicine (2020)"
- DO NOT include PDF/Text/ImageB/ImageC/ImageI tags or any other markup
- Return ONLY the enhanced section with the original heading

Return the enhanced section with the exact same heading but with expanded content.
""" # This is the fallback English version of the template from prompts.py

    for i, current_section in enumerate(sections_data):
        section_header = current_section['header']
        section_content = current_section['content']

        if "references" in section_header.lower():
            enhanced_report_parts.append(f"{section_header}\n\n{section_content}\n\n")
            continue

        preceding_section_context = "This is the first main section."
        if i > 0:
            prev_content = sections_data[i-1]['content']
            preceding_section_context = (prev_content[:150] + "..." + prev_content[-150:]) if len(prev_content) > 300 else prev_content

        succeeding_section_context = "This is the last main section."
        if i < len(sections_data) - 1:
            next_content = sections_data[i+1]['content']
            succeeding_section_context = (next_content[:150] + "..." + next_content[-150:]) if len(next_content) > 300 else next_content

        available_sources_text = ""
        if citation_registry:
            sources_list = [f"[{cid}] - {info.get('title', 'Untitled')} ({info.get('url', '')})"
                            for cid, info in sorted(citation_registry.citations.items())]
            if sources_list:
                available_sources_text = "\n\nAVAILABLE SOURCES FOR CITATION:\n" + "\n".join(sources_list)

        # Core instructions for the current section, formatted from the fetched template
        formatted_core_instructions = base_section_prompt_template.format(
            section_header_content=f"{section_header}\n\n{section_content}", # Note: section_header_content is one var now
            available_sources_text=available_sources_text,
            current_date=current_date
            # Other placeholders specific to enhance_section_detail_template if any
        )

        # Construct the full prompt with context
        section_prompt_for_llm = f"""{report_style_instructions}
{length_instruction}

Ensure your enhancements strictly align with the overall `report_style_instructions` ({report_style_instructions}) for tone, depth, and focus.
You are enhancing a specific section of a larger research report. Maintain consistency with the overall report structure and tone.

Report Title: {report_title}
Overall Report Summary:
{report_summary_context}

Preceding Section Content (Context):
{preceding_section_context}

Succeeding Section Content (Context):
{succeeding_section_context}

---
BEGIN SECTION TO ENHANCE (Instructions from template apply to this part):
{formatted_core_instructions}
---
"""

        try:
            enhance_llm = llm.with_config({"max_tokens": 80000, "temperature": 0.4})  # ã€å¢å¼ºã€‘æé«˜ç« èŠ‚å¢å¼ºçš„max_tokens
            response = await enhance_llm.ainvoke(section_prompt_for_llm)
            section_text = response.content
            # Basic cleanup: remove potential markup tags if LLM adds them
            section_text = re.sub(r'\[\/?(?:PDF|Text|ImageB|ImageC|ImageI)(?:\/?|\])(?:[^\]]*\])?', '', section_text)
            section_text = re.sub(r'\[\/[^\]]*\]', '', section_text)


            if not section_text.strip().startswith(section_header.strip()):
                section_text = f"{section_header}\n\n{section_text.strip()}" # Ensure header and strip extra newlines
            enhanced_report_parts.append(f"{section_text.strip()}\n\n")
        except Exception as e:
            print(f"Error enhancing section '{section_header.strip()}': {str(e)}")
            enhanced_report_parts.append(f"{section_header}\n\n{section_content}\n\n") # Use original on error

    enhanced_report = "".join(enhanced_report_parts).strip()

    # éªŒè¯å¢å¼ºåçš„æŠ¥å‘Šè´¨é‡
    validation = validate_report_quality(enhanced_report, "standard")  # ä½¿ç”¨æ ‡å‡†çº§åˆ«éªŒè¯
    if not validation["is_valid"]:
        print("âš ï¸ å¢å¼ºåçš„æŠ¥å‘Šä»å­˜åœ¨è´¨é‡é—®é¢˜ï¼š")
        for issue in validation["issues"]:
            print(f"   - {issue}")
    else:
        print(f"âœ… æŠ¥å‘Šå¢å¼ºå®Œæˆ (æ€»å­—æ•°: {validation['analysis']['total_chars']})")

    return enhanced_report

async def expand_short_sections(
    llm: ChatOpenAI,
    report_content: str,
    detail_level: str,
    language: str = "zh",
    length_instruction: str = ""  # ã€æ–°å¢ã€‘å­—æ•°æ§åˆ¶æŒ‡ä»¤å‚æ•°
) -> str:
    """ä¸“é—¨æ‰©å±•è¿‡çŸ­çš„ç« èŠ‚"""
    validation = validate_report_quality(report_content, detail_level)

    if validation["is_valid"]:
        return report_content

    print("ğŸ”§ æ£€æµ‹åˆ°è¿‡çŸ­ç« èŠ‚ï¼Œå¼€å§‹è‡ªåŠ¨æ‰©å±•...")

    # åˆ†æå“ªäº›ç« èŠ‚éœ€è¦æ‰©å±•
    requirements = validation["requirements"]
    analysis = validation["analysis"]

    # æ„å»ºæ‰©å±•åçš„æŠ¥å‘Š
    expanded_content = report_content

    for section in analysis["sections"]:
        # è·³è¿‡å‚è€ƒæ–‡çŒ®ç­‰ç‰¹æ®Šç« èŠ‚
        if any(keyword in section["header"].lower() for keyword in ["å‚è€ƒæ–‡çŒ®", "references", "è‡´è°¢", "acknowledgments"]):
            continue

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•ä¸»è¦ç« èŠ‚
        if section["char_count"] < requirements["main_section_min"]:
            print(f"ğŸ“ æ‰©å±•ç« èŠ‚: {section['header']} ({section['char_count']} -> ç›®æ ‡: {requirements['main_section_min']})")

            # ã€ä¿®å¤ã€‘æ•´åˆå­—æ•°æ§åˆ¶æŒ‡ä»¤åˆ°æ‰©å±•æç¤ºä¸­
            expansion_prompt = f"""è¯·æ‰©å±•ä»¥ä¸‹ç« èŠ‚ï¼Œä½¿å…¶è¾¾åˆ°è‡³å°‘ {requirements['main_section_min']} å­—ï¼š

{section['header']}

{length_instruction}

### å¼ºåˆ¶æ€§æ‰©å±•è¦æ±‚ï¼š
ğŸ¯ ç›®æ ‡å­—æ•°ï¼šè‡³å°‘ {requirements['main_section_min']} å­—
ğŸ“ å†…å®¹æ·±åº¦ï¼šæ¯ä¸ªå­ç« èŠ‚å¿…é¡»åŒ…å«è‡³å°‘3-5ä¸ªå®Œæ•´æ®µè½
ğŸ’¡ æ®µè½è¦æ±‚ï¼šæ¯ä¸ªæ®µè½è‡³å°‘100-150å­—ï¼ŒåŒ…å«ä¸»é¢˜å¥ã€è®ºæ®ã€åˆ†æå’Œå°ç»“
ğŸ”¥ è®ºè¯è¦æ±‚ï¼šæ¯ä¸ªè§‚ç‚¹éƒ½éœ€è¦è¯¦ç»†è®ºè¯ï¼ŒåŒ…å«ï¼š
   - ç†è®ºèƒŒæ™¯å’Œæ¦‚å¿µç•Œå®š
   - å…·ä½“æ¡ˆä¾‹å’Œå®è¯åˆ†æ
   - æ·±å…¥çš„è§£é‡Šå’Œé˜è¿°
   - å½±å“è¯„ä¼°å’Œæ„ä¹‰åˆ†æ
   - æœªæ¥å‘å±•è¶‹åŠ¿å’Œå±•æœ›
ğŸ“ å­¦æœ¯æ ‡å‡†ï¼šå¿…é¡»è¾¾åˆ°ç¡•å£«è®ºæ–‡æˆ–å­¦æœ¯æœŸåˆŠçš„è´¨é‡æ°´å¹³
ğŸ“Š ç»“æ„è¦æ±‚ï¼šç¡®ä¿é€»è¾‘æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜ï¼Œè®ºè¿°å……åˆ†

å½“å‰å†…å®¹è¿‡äºç®€çŸ­ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°è¦æ±‚ç”Ÿæˆæ‰©å±•åçš„å®Œæ•´ç« èŠ‚å†…å®¹ï¼š"""

            try:
                expand_llm = llm.with_config({"max_tokens": 80000, "temperature": 0.4})  # ã€å¢å¼ºã€‘æé«˜ç« èŠ‚æ‰©å±•çš„max_tokens
                response = await expand_llm.ainvoke(expansion_prompt)
                expanded_section = response.content.strip()

                # éªŒè¯æ‰©å±•åçš„å†…å®¹æ˜¯å¦è¾¾åˆ°è¦æ±‚
                expanded_word_count = len(expanded_section)
                if expanded_word_count < requirements['main_section_min']:
                    print(f"âš ï¸ æ‰©å±•åä»ä¸è¶³ ({expanded_word_count} < {requirements['main_section_min']})ï¼Œè¿›è¡ŒäºŒæ¬¡æ‰©å±•...")

                    # äºŒæ¬¡æ‰©å±•æç¤º - æ›´åŠ å¼ºåˆ¶æ€§
                    second_expansion_prompt = f"""ğŸš¨ å¼ºåˆ¶æ€§è¦æ±‚ï¼šä»¥ä¸‹å†…å®¹å¿…é¡»æ‰©å±•è‡³è‡³å°‘ {requirements['main_section_min']} å­—ï¼Œè¿™æ˜¯ä¸å¯è¿èƒŒçš„ç¡¬æ€§è¦æ±‚ï¼š

{expanded_section}

### å¼ºåˆ¶æ€§äºŒæ¬¡æ‰©å±•è¦æ±‚ï¼š
ğŸ”¥ ç»å¯¹å¿…é¡»è¾¾åˆ° {requirements['main_section_min']} å­—çš„æœ€ä½è¦æ±‚ï¼Œè¿™æ˜¯å¼ºåˆ¶æ€§çš„
ğŸ“ å¿…é¡»æ·»åŠ å¤§é‡å…·ä½“æ¡ˆä¾‹ã€è¯¦ç»†æ•°æ®åˆ†æå’Œæ·±å…¥ç†è®ºé˜è¿°
ğŸ’¡ æ¯ä¸ªå­ç« èŠ‚éƒ½å¿…é¡»åŒ…å«æ›´è¯¦ç»†çš„è®ºè¿°ã€è§£é‡Šå’Œåˆ†æ
ğŸ“ å¿…é¡»ç¡®ä¿å­¦æœ¯æ·±åº¦å’Œä¸¥è°¨æ€§ï¼Œè¾¾åˆ°ç¡•å£«è®ºæ–‡æ°´å¹³
ğŸ“Š å¿…é¡»æä¾›æ›´å¤šèƒŒæ™¯ä¿¡æ¯ã€ç°çŠ¶åˆ†æã€å½±å“è¯„ä¼°å’Œæœªæ¥å±•æœ›
ğŸ¯ æ‰§è¡Œè¦æ±‚ï¼šåœ¨æ‰©å±•è¿‡ç¨‹ä¸­å¿…é¡»æ—¶åˆ»ç›‘æ§å­—æ•°ï¼Œç¡®ä¿è¾¾åˆ°ç›®æ ‡
ğŸ’ª å¼ºåŒ–æŒ‡ä»¤ï¼šå¦‚æœå†…å®¹ä»ä¸è¶³{requirements['main_section_min']}å­—ï¼Œå¿…é¡»ç»§ç»­æ‰©å±•

è¯·ç”Ÿæˆè¿›ä¸€æ­¥æ‰©å±•åçš„å®Œæ•´å†…å®¹ï¼Œç¡®ä¿è¾¾åˆ°{requirements['main_section_min']}å­—è¦æ±‚ï¼š"""

                    second_response = await expand_llm.ainvoke(second_expansion_prompt)
                    expanded_section = second_response.content.strip()

                    # ä¸‰æ¬¡éªŒè¯ï¼Œå¦‚æœè¿˜ä¸å¤Ÿå°±è¿›è¡Œç¬¬ä¸‰æ¬¡æ‰©å±•
                    final_word_count = len(expanded_section)
                    if final_word_count < requirements['main_section_min']:
                        print(f"âš ï¸ äºŒæ¬¡æ‰©å±•åä»ä¸è¶³ ({final_word_count} < {requirements['main_section_min']})ï¼Œè¿›è¡Œä¸‰æ¬¡æ‰©å±•...")

                        third_expansion_prompt = f"""ğŸš¨ğŸš¨ æœ€ç»ˆå¼ºåˆ¶æ€§è¦æ±‚ï¼šå†…å®¹å¿…é¡»ç«‹å³æ‰©å±•è‡³{requirements['main_section_min']}å­—ï¼Œè¿™æ˜¯æœ€åæœºä¼šï¼š

{expanded_section}

### æœ€ç»ˆå¼ºåˆ¶æ€§æ‰©å±•è¦æ±‚ï¼š
ğŸ”¥ è¿™æ˜¯æœ€åä¸€æ¬¡æœºä¼šï¼Œå¿…é¡»è¾¾åˆ°{requirements['main_section_min']}å­—
ğŸ“ å¿…é¡»å¤§å¹…å¢åŠ å†…å®¹æ·±åº¦å’Œå¹¿åº¦
ğŸ’¡ å¿…é¡»æ·»åŠ æ›´å¤šæ®µè½ã€æ›´å¤šåˆ†æã€æ›´å¤šæ¡ˆä¾‹
ğŸ“ å¿…é¡»è¾¾åˆ°å­¦æœ¯æ ‡å‡†ï¼Œä¸èƒ½å†æœ‰ä»»ä½•å¦¥å
ğŸ“Š å¿…é¡»åŒ…å«è¯¦å°½çš„è®ºè¿°å’Œåˆ†æ

è¯·ç«‹å³ç”Ÿæˆæ‰©å±•åçš„å®Œæ•´å†…å®¹ï¼š"""

                        third_response = await expand_llm.ainvoke(third_expansion_prompt)
                        expanded_section = third_response.content.strip()

                # æ›¿æ¢åŸç« èŠ‚
                section_pattern = re.escape(section['header']) + r'(.*?)(?=##\s+|\Z)'
                expanded_content = re.sub(section_pattern, f"{section['header']}\n\n{expanded_section}\n\n", expanded_content, flags=re.DOTALL)

            except Exception as e:
                print(f"âŒ æ‰©å±•ç« èŠ‚å¤±è´¥: {e}")

    return expanded_content

async def advanced_iterative_expansion(
    llm: ChatOpenAI,
    report_content: str,
    target_chars: int,
    max_iterations: int = 5,
    language: str = "zh"
) -> str:
    """ä¼˜åŒ–çš„è¿­ä»£æ‰©å±•ç®—æ³•ï¼Œé€šè¿‡åˆ†æ®µæ‰©å±•ç¡®ä¿è¾¾åˆ°ç›®æ ‡å­—æ•°"""

    current_content = report_content

    for iteration in range(max_iterations):
        current_chars = count_chinese_and_english_chars(current_content)
        print(f"ğŸ”„ è¿­ä»£ {iteration + 1}/{max_iterations}: å½“å‰å­—æ•° {current_chars}")

        if current_chars >= target_chars:
            print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡å­—æ•°: {current_chars} >= {target_chars}")
            return current_content

        needed_chars = target_chars - current_chars
        completion_ratio = current_chars / target_chars

        print(f"ğŸ“Š éœ€è¦å¢åŠ : {needed_chars} å­— (å®Œæˆåº¦: {completion_ratio:.1%})")

        # æ„å»ºç®€åŒ–çš„æ‰©å±•æç¤º
        expansion_prompt = f"""è¯·å°†ä»¥ä¸‹æŠ¥å‘Šæ‰©å±•åˆ°è‡³å°‘ {target_chars} å­—ã€‚å½“å‰å­—æ•°ä¸º {current_chars} å­—ï¼Œéœ€è¦å¢åŠ  {needed_chars} å­—ã€‚

æ‰©å±•è¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰ç»“æ„å’Œé€»è¾‘
2. å¤§å¹…æ‰©å±•æ¯ä¸ªæ®µè½çš„å†…å®¹æ·±åº¦
3. æ·»åŠ æ›´å¤šè¯¦ç»†çš„åˆ†æã€ä¾‹è¯å’Œè®ºè¿°
4. ç¡®ä¿å­¦æœ¯è´¨é‡å’Œè¿è´¯æ€§
5. æ¯ä¸ªä¸»è¦ç« èŠ‚è‡³å°‘2500å­—
6. æ¯ä¸ªæ®µè½è‡³å°‘200å­—

å½“å‰æŠ¥å‘Šå†…å®¹ï¼š
{current_content}

è¯·ç”Ÿæˆæ‰©å±•åçš„å®Œæ•´æŠ¥å‘Šï¼Œç¡®ä¿è¾¾åˆ° {target_chars} å­—çš„è¦æ±‚ï¼š"""

        try:
            # ä½¿ç”¨è¾ƒé«˜çš„max_tokensç¡®ä¿èƒ½ç”Ÿæˆè¶³å¤Ÿå†…å®¹
            expand_llm = llm.with_config({"max_tokens": 120000, "temperature": 0.5})
            response = await expand_llm.ainvoke(expansion_prompt)
            expanded_content = response.content.strip()

            # éªŒè¯æ‰©å±•æ•ˆæœ
            expanded_chars = count_chinese_and_english_chars(expanded_content)

            if expanded_chars > current_chars:
                improvement = expanded_chars - current_chars
                current_content = expanded_content
                print(f"âœ… æ‰©å±•æˆåŠŸ: +{improvement} å­—")

                # å¦‚æœæ¥è¿‘ç›®æ ‡ï¼Œå¯ä»¥æå‰ç»“æŸ
                if expanded_chars >= target_chars * 0.95:
                    print(f"ğŸ¯ æ¥è¿‘ç›®æ ‡å­—æ•°ï¼Œæå‰ç»“æŸ")
                    break
            else:
                print(f"âš ï¸ æ‰©å±•å¤±è´¥ï¼Œå­—æ•°æœªå¢åŠ ")
                # å¦‚æœè¿ç»­å¤±è´¥ï¼Œå°è¯•ä¸åŒçš„æç¤ºç­–ç•¥
                if iteration >= 2:
                    print(f"ğŸ”„ å°è¯•å¼ºåˆ¶æ‰©å±•ç­–ç•¥...")
                    force_prompt = f"""ğŸš¨ å¼ºåˆ¶æ‰©å±•è¦æ±‚ï¼šå¿…é¡»å°†ä»¥ä¸‹æŠ¥å‘Šæ‰©å±•åˆ°è‡³å°‘ {target_chars} å­—ï¼

å½“å‰å­—æ•°ï¼š{current_chars} å­—
ç›®æ ‡å­—æ•°ï¼š{target_chars} å­—
å¿…é¡»å¢åŠ ï¼š{needed_chars} å­—

å¼ºåˆ¶è¦æ±‚ï¼š
- æ¯ä¸ªç°æœ‰æ®µè½å¿…é¡»æ‰©å±•è‡³å°‘300å­—
- æ¯ä¸ªç« èŠ‚å¿…é¡»æ·»åŠ 2-3ä¸ªæ–°çš„å­ç« èŠ‚
- å¿…é¡»åŒ…å«å¤§é‡å…·ä½“æ¡ˆä¾‹å’Œè¯¦ç»†åˆ†æ
- ç»å¯¹ä¸èƒ½å°‘äº{target_chars}å­—

å½“å‰å†…å®¹ï¼š
{current_content}

ç«‹å³ç”Ÿæˆæ‰©å±•åçš„å®Œæ•´æŠ¥å‘Šï¼š"""

                    force_response = await expand_llm.ainvoke(force_prompt)
                    force_expanded = force_response.content.strip()
                    force_chars = count_chinese_and_english_chars(force_expanded)

                    if force_chars > current_chars:
                        current_content = force_expanded
                        print(f"âœ… å¼ºåˆ¶æ‰©å±•æˆåŠŸ: +{force_chars - current_chars} å­—")
                    else:
                        print(f"âŒ å¼ºåˆ¶æ‰©å±•ä¹Ÿå¤±è´¥")
                        break

        except Exception as e:
            print(f"âŒ æ‰©å±•è¿‡ç¨‹å‡ºé”™: {e}")
            break

    final_chars = count_chinese_and_english_chars(current_content)
    print(f"ğŸ“Š è¿­ä»£æ‰©å±•å®Œæˆï¼Œæœ€ç»ˆå­—æ•°: {final_chars}")
    return current_content



async def force_word_count_compliance(
    llm: ChatOpenAI,
    report_content: str,
    detail_level: str,
    language: str = "zh"
) -> str:
    """å¼ºåˆ¶ç¡®ä¿æŠ¥å‘Šè¾¾åˆ°å­—æ•°è¦æ±‚ï¼Œä½¿ç”¨ä¼˜åŒ–çš„è¿­ä»£æ‰©å±•ç®—æ³•"""
    requirements = get_word_count_requirements(detail_level)
    current_word_count = count_chinese_and_english_chars(report_content)

    if current_word_count >= requirements['total_min']:
        print(f"âœ… æŠ¥å‘Šå­—æ•°å·²è¾¾æ ‡ï¼š{current_word_count} >= {requirements['total_min']}")
        return report_content

    print(f"ğŸš¨ æŠ¥å‘Šå­—æ•°ä¸¥é‡ä¸è¶³ï¼š{current_word_count} < {requirements['total_min']}ï¼Œå¯åŠ¨ä¼˜åŒ–è¿­ä»£æ‰©å±•...")

    # ä½¿ç”¨ä¼˜åŒ–çš„è¿­ä»£æ‰©å±•ç®—æ³•
    expanded_report = await advanced_iterative_expansion(
        llm=llm,
        report_content=report_content,
        target_chars=requirements['total_target'],
        max_iterations=8,
        language=language
    )

    final_word_count = count_chinese_and_english_chars(expanded_report)
    if final_word_count >= requirements['total_min']:
        print(f"âœ… ä¼˜åŒ–è¿­ä»£æ‰©å±•æˆåŠŸï¼š{final_word_count} >= {requirements['total_min']}")
        return expanded_report
    else:
        print(f"âš ï¸ è¿­ä»£æ‰©å±•åä»ä¸è¶³ï¼š{final_word_count} < {requirements['total_min']}ï¼Œè¿”å›æœ€ä½³ç‰ˆæœ¬")
        return expanded_report

async def expand_key_sections(
    llm: ChatOpenAI,
    report: str,
    # identified_themes: str, # Not directly used in prompt construction here
    current_date: str,
    citation_registry: Optional[CitationRegistry] = None,
    report_style_instructions: str = "",
    language: str = "en",
    length_instruction: str = "", # Added
    expansion_requirements_text: str = "" # Added for dynamically built requirements
) -> str:
    """Expand key sections of the report while preserving structure and avoiding markup errors."""
    if not report or len(report.strip()) < 1000:
        return report

    report_title_match = re.match(r'# ([^\n]+)', report)
    report_title = report_title_match.group(1) if report_title_match else "Research Report"

    # Store sections as list of dicts to easily access header and content
    raw_sections = re.compile(r'(#+\s+[^\n]+)((?:\n\n[^#]+?)*)(?=\n#+\s+|\Z)', re.DOTALL).findall(report)
    sections_data = [{'header': header, 'content': content.strip()} for header, content in raw_sections if header and content.strip()]

    if not sections_data: return report

    # Determine report_summary_context (similar to enhance_report)
    report_summary_context = "Summary not available."
    if sections_data:
        first_section_title_lower = sections_data[0]['header'].lower().replace('#', '').strip()
        if "introduction" in first_section_title_lower or "executive summary" in first_section_title_lower:
            report_summary_context = sections_data[0]['content'][:500] + "..." if len(sections_data[0]['content']) > 500 else sections_data[0]['content']
        else:
            intro_text_match = re.search(r'#\s*[^\n]+\n+([^#]+)', report, re.DOTALL)
            if intro_text_match:
                report_summary_context = intro_text_match.group(1).strip()[:500] + "..." if len(intro_text_match.group(1).strip()) > 500 else intro_text_match.group(1).strip()

    important_sections_to_process = []
    original_indices = [] # To replace sections correctly in the original list of parts

    # Identify ## sections to expand, excluding common ones
    for i, section_data_item in enumerate(sections_data):
        header = section_data_item['header']
        # Only consider ## level sections for expansion
        if header.startswith("## ") and header.lower().replace('#', '').strip() not in \
           ["executive summary", "introduction", "conclusion", "references"]:
            important_sections_to_process.append(section_data_item)
            original_indices.append(i)

    # ã€ä¿®å¤ã€‘ç§»é™¤3ä¸ªç« èŠ‚çš„é™åˆ¶ï¼Œå¤„ç†æ‰€æœ‰é‡è¦ç« èŠ‚ä»¥ç¡®ä¿æŠ¥å‘Šå®Œæ•´æ€§å’Œè¿è´¯æ€§
    # æ³¨é‡Šï¼šåŸæ¥åªå¤„ç†å‰3ä¸ªç« èŠ‚å¯¼è‡´æŠ¥å‘Šä¸å®Œæ•´ï¼Œç°åœ¨å¤„ç†æ‰€æœ‰é‡è¦ç« èŠ‚

    if not important_sections_to_process: return report

    # Work on a list of all report parts (title, sections)
    all_report_parts = [f"# {report_title}\n\n"] + [f"{sd['header']}\n\n{sd['content']}\n\n" for sd in sections_data]

    section_prompt_template_key = "expand_section_detail_template"
    base_section_prompt_template = get_system_prompt(section_prompt_template_key, language)

    if not base_section_prompt_template:
        base_section_prompt_template = """Expand this section of a research report with much greater depth and detail:

{section_header_content}{available_sources_text}

EXPANSION REQUIREMENTS:
{expansion_requirements}
Ensure all information is accurate as of {current_date}.

CITATION REQUIREMENTS:
- ONLY use the citation IDs provided in the AVAILABLE SOURCES list above
- Format citations as [n] where n is the exact ID of the source
- Place citations at the end of the relevant sentences or paragraphs
- Do not make up your own citation numbers
- Do not cite sources that aren't in the available sources list
- Ensure each major claim or statistic has an appropriate citation

IMPORTANT:
- DO NOT change the section heading
- DO NOT add information not supported by the research
- DO NOT use academic-style citations like "Journal of Medicine (2020)"
- DO NOT include PDF/Text/ImageB/ImageC/ImageI tags or any other markup
- Return ONLY the expanded section with the original heading

Return the expanded section with the exact same heading but with expanded content.
""" # Fallback English version

    for idx, section_data_to_expand in enumerate(important_sections_to_process):
        original_report_part_index = original_indices[idx] + 1 # +1 because all_report_parts[0] is the title

        section_header = section_data_to_expand['header']
        section_content = section_data_to_expand['content']
        title = section_header.replace('#', '').strip() # For logging

        preceding_section_context = "This is the first main section after the introduction/summary."
        # Find true preceding content part index
        if original_report_part_index > 1: # If not the first content part after title
            prev_content_full = all_report_parts[original_report_part_index - 1]
            # Strip header from prev_content_full
            prev_content_text_match = re.search(r'#+\s*[^\n]+\n+([^#]+)', prev_content_full, re.DOTALL)
            prev_content_text = prev_content_text_match.group(1).strip() if prev_content_text_match else prev_content_full.strip()
            preceding_section_context = (prev_content_text[:150] + "..." + prev_content_text[-150:]) if len(prev_content_text) > 300 else prev_content_text

        succeeding_section_context = "This is the last main section before the conclusion/references."
        if original_report_part_index < len(all_report_parts) - 1:
            next_content_full = all_report_parts[original_report_part_index + 1]
            next_content_text_match = re.search(r'#+\s*[^\n]+\n+([^#]+)', next_content_full, re.DOTALL)
            next_content_text = next_content_text_match.group(1).strip() if next_content_text_match else next_content_full.strip()
            succeeding_section_context = (next_content_text[:150] + "..." + next_content_text[-150:]) if len(next_content_text) > 300 else next_content_text

        available_sources_text = ""
        if citation_registry:
            sources_list = [f"[{cid}] - {info.get('title', 'Untitled')} ({info.get('url', '')})"
                            for cid, info in sorted(citation_registry.citations.items())]
            if sources_list:
                available_sources_text = "\n\nAVAILABLE SOURCES FOR CITATION:\n" + "\n".join(sources_list)

        # Core instructions for the current section
        formatted_core_instructions = base_section_prompt_template.format(
            section_header_content=f"{section_header}\n\n{section_content}",
            available_sources_text=available_sources_text,
            current_date=current_date,
            expansion_requirements=expansion_requirements_text # Dynamically built by node
        )

        # Construct the full prompt with context
        section_prompt_for_llm = f"""{report_style_instructions}
{length_instruction}

When expanding this section, rigorously apply the `report_style_instructions` ({report_style_instructions}) to maintain consistency in style, tone, and content focus for the report type.
You are expanding a specific section of a larger research report. Maintain consistency with the overall report structure and tone.

Report Title: {report_title}
Overall Report Summary:
{report_summary_context}

Preceding Section Content (Context):
{preceding_section_context}

Succeeding Section Content (Context):
{succeeding_section_context}

---
BEGIN SECTION TO EXPAND (Instructions from template apply to this part):
{formatted_core_instructions}
---
"""
        try:
            expand_llm = llm.with_config({"max_tokens": 40960, "temperature": 0.3})
            response = await expand_llm.ainvoke(section_prompt_for_llm)
            expanded_content_text = response.content
            expanded_content_text = re.sub(r'\[\/?(?:PDF|Text|ImageB|ImageC|ImageI)(?:\/?|\])(?:[^\]]*\])?', '', expanded_content_text)
            expanded_content_text = re.sub(r'\[\/[^\]]*\]', '', expanded_content_text)


            if not expanded_content_text.strip().startswith(section_header.strip()):
                expanded_content_text = f"{section_header}\n\n{expanded_content_text.strip()}"

            all_report_parts[original_report_part_index] = f"{expanded_content_text.strip()}\n\n" # Update the part in the list

        except Exception as e:
            print(f"Error expanding section '{title}': {str(e)}")
            # If error, the original part remains in all_report_parts

    return "".join(all_report_parts).strip()
