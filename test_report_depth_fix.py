#!/usr/bin/env python3
"""
æµ‹è¯•æŠ¥å‘Šæ·±åº¦ä¿®å¤æ•ˆæœçš„è„šæœ¬
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from shandu.agents.nodes.report_generation import _get_length_instruction

def test_length_instructions():
    """æµ‹è¯•å­—æ•°æ§åˆ¶æŒ‡ä»¤æ˜¯å¦åŒ…å«æ·±åº¦è¦æ±‚"""
    print("=== æµ‹è¯•å­—æ•°æ§åˆ¶æŒ‡ä»¤ ===\n")
    
    # æµ‹è¯•ä¸åŒçš„è¯¦ç»†ç¨‹åº¦
    detail_levels = ["brief", "standard", "detailed", "custom_15000"]
    
    for level in detail_levels:
        print(f"ğŸ“‹ è¯¦ç»†ç¨‹åº¦: {level}")
        instruction = _get_length_instruction(level)
        print(f"æŒ‡ä»¤å†…å®¹:\n{instruction}\n")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ·±åº¦è¦æ±‚
        depth_indicators = [
            "å†…å®¹æ·±åº¦",
            "è´¨é‡è¦æ±‚", 
            "å®Œæ•´æ®µè½",
            "è®ºè¯è¦æ±‚",
            "å­¦æœ¯æ ‡å‡†"
        ]
        
        found_indicators = [indicator for indicator in depth_indicators if indicator in instruction]
        print(f"âœ… åŒ…å«æ·±åº¦æŒ‡æ ‡: {', '.join(found_indicators)}")
        print("-" * 50)

def analyze_current_report():
    """åˆ†æå½“å‰æŠ¥å‘Šçš„é—®é¢˜"""
    print("\n=== åˆ†æå½“å‰æŠ¥å‘Šé—®é¢˜ ===\n")
    
    try:
        with open("output/xiyou_report.md", "r", encoding="utf-8") as f:
            content = f.read()
        
        # ç»Ÿè®¡å­—æ•°
        word_count = len(content)
        print(f"ğŸ“Š å½“å‰æŠ¥å‘Šæ€»å­—æ•°: {word_count}")
        
        # åˆ†æç« èŠ‚ç»“æ„
        import re
        
        # æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚
        sections = re.findall(r'(#{2,4}\s+[^\n]+)', content)
        print(f"ğŸ“‹ ç« èŠ‚æ€»æ•°: {len(sections)}")
        
        # åˆ†æå­ç« èŠ‚å†…å®¹é•¿åº¦
        subsections = re.findall(r'(#{3,4}\s+[^\n]+)(.*?)(?=#{2,4}\s+|\Z)', content, re.DOTALL)
        
        short_sections = []
        for header, content_part in subsections:
            content_length = len(content_part.strip())
            if content_length < 200:  # å°‘äº200å­—ç¬¦çš„ç« èŠ‚
                short_sections.append((header.strip(), content_length))
        
        print(f"âš ï¸  è¿‡çŸ­çš„å­ç« èŠ‚æ•°é‡: {len(short_sections)}")
        for header, length in short_sections[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"   - {header}: {length}å­—ç¬¦")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åªæœ‰ä¸€å¥è¯çš„ç« èŠ‚
        one_sentence_sections = []
        for header, content_part in subsections:
            sentences = content_part.strip().split('ã€‚')
            if len([s for s in sentences if s.strip()]) <= 1:
                one_sentence_sections.append(header.strip())
        
        print(f"ğŸš¨ åªæœ‰ä¸€å¥è¯çš„ç« èŠ‚æ•°é‡: {len(one_sentence_sections)}")
        for header in one_sentence_sections[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"   - {header}")
            
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æŠ¥å‘Šæ–‡ä»¶ output/xiyou_report.md")

def test_word_count_validation():
    """æµ‹è¯•å­—æ•°éªŒè¯åŠŸèƒ½"""
    print("\n=== æµ‹è¯•å­—æ•°éªŒè¯åŠŸèƒ½ ===\n")

    try:
        from shandu.agents.processors.report_generator import (
            get_word_count_requirements,
            validate_report_quality,
            analyze_report_structure
        )

        # æµ‹è¯•å­—æ•°è¦æ±‚è·å–
        for level in ["brief", "standard", "detailed", "custom_15000"]:
            requirements = get_word_count_requirements(level)
            print(f"ğŸ“‹ {level} çº§åˆ«è¦æ±‚:")
            print(f"   æ€»å­—æ•°: {requirements['total_min']}-{requirements['total_max']}")
            print(f"   ä¸»ç« èŠ‚: è‡³å°‘{requirements['main_section_min']}å­—")
            print(f"   å­ç« èŠ‚: è‡³å°‘{requirements['sub_section_min']}å­—")
            print()

        # æµ‹è¯•å½“å‰æŠ¥å‘ŠéªŒè¯
        if os.path.exists("output/xiyou_report.md"):
            with open("output/xiyou_report.md", "r", encoding="utf-8") as f:
                content = f.read()

            validation = validate_report_quality(content, "detailed")
            print(f"ğŸ“Š å½“å‰æŠ¥å‘ŠéªŒè¯ç»“æœ:")
            print(f"   æ˜¯å¦åˆæ ¼: {'âœ…' if validation['is_valid'] else 'âŒ'}")
            print(f"   æ€»å­—æ•°: {validation['analysis']['total_words']}")
            print(f"   ç« èŠ‚æ•°: {validation['analysis']['section_count']}")

            if validation['issues']:
                print("   é—®é¢˜åˆ—è¡¨:")
                for issue in validation['issues']:
                    print(f"     - {issue}")

            if validation['warnings']:
                print("   è­¦å‘Šåˆ—è¡¨:")
                for warning in validation['warnings']:
                    print(f"     - {warning}")

        print("âœ… å­—æ•°éªŒè¯åŠŸèƒ½æµ‹è¯•å®Œæˆ")

    except ImportError as e:
        print(f"âŒ å¯¼å…¥éªŒè¯å‡½æ•°å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æŠ¥å‘Šæ·±åº¦ä¿®å¤æµ‹è¯•\n")

    # æµ‹è¯•å­—æ•°æ§åˆ¶æŒ‡ä»¤
    test_length_instructions()

    # æµ‹è¯•å­—æ•°éªŒè¯åŠŸèƒ½
    test_word_count_validation()

    # åˆ†æå½“å‰æŠ¥å‘Š
    analyze_current_report()

    print("\n=== ä¿®å¤æ€»ç»“ ===")
    print("âœ… å·²å¢å¼ºå­—æ•°æ§åˆ¶æŒ‡ä»¤ï¼ŒåŒ…å«å…·ä½“çš„å†…å®¹æ·±åº¦è¦æ±‚")
    print("âœ… å·²æé«˜LLMçš„max_tokensé™åˆ¶ï¼Œæ”¯æŒæ›´é•¿çš„å†…å®¹ç”Ÿæˆ")
    print("âœ… å·²åœ¨ç³»ç»Ÿæç¤ºä¸­å¼ºåŒ–å­¦æœ¯è´¨é‡è¦æ±‚")
    print("âœ… å·²åœ¨ç”¨æˆ·æ¶ˆæ¯ä¸­æ·»åŠ å¼ºåˆ¶æ€§å†…å®¹æ·±åº¦è¦æ±‚")
    print("âœ… å·²æ·»åŠ Pythonå±‚é¢çš„å­—æ•°éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤æœºåˆ¶")
    print("âœ… å·²é›†æˆè‡ªåŠ¨æ‰©å±•è¿‡çŸ­ç« èŠ‚çš„åŠŸèƒ½")
    print("\nğŸ¯ å»ºè®®é‡æ–°ç”ŸæˆæŠ¥å‘Šä»¥éªŒè¯ä¿®å¤æ•ˆæœ")

if __name__ == "__main__":
    main()
